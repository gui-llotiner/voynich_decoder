{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Word and Glyph Displacement\n",
    "\n",
    "This notebook processes the Voynich Manuscript text (folio 1r from the ZL transliteration) to analyze word similarities using Levenshtein distance, the goal is to find any displacement rules that unveils any pattern for word and/setence generator. It performs the following:\n",
    "\n",
    "- **Section I**: Loads and tokenizes words, treating 'ch' and 'sh' as digraphs.\n",
    "- **Section II**: Computes Levenshtein distances for all unique word pairs.\n",
    "- **Section III**: Computes closest word pairs (excluding identicals) with detailed alignments and operation counts.\n",
    "- **Section IV**: Calculates distances to each word's predecessor in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "\n",
    "class VoynichTextProcessor:\n",
    "    \"\"\"Processes Voynich Manuscript text with folio-aware cleaning and tokenization.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.digraphs = {'ch', 'sh'}\n",
    "        self.folio_data = {}\n",
    "        self.raw_text = None\n",
    "\n",
    "    def load_raw_text(self, filepath: str) -> bool:\n",
    "        \"\"\"Load raw text from file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                self.raw_text = f.read()\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{filepath}' not found.\")\n",
    "            return False\n",
    "\n",
    "    def tokenize_word(self, word: str) -> List[str]:\n",
    "        \"\"\"Tokenize a word into characters or digraphs (e.g., 'ch', 'sh').\"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i:i+2] in self.digraphs:\n",
    "                tokens.append(word[i:i+2])\n",
    "                i += 2\n",
    "            else:\n",
    "                tokens.append(word[i])\n",
    "                i += 1\n",
    "        return tokens\n",
    "\n",
    "    def parse_all_folios(self, filepath: str, glyph_level: bool = False, treat_commas_as_spaces: bool = True, min_word_length: int = 2) -> Tuple[Dict, List[str], List[List[str]]]:\n",
    "        \"\"\"Parse Voynich text into folios and sentences.\"\"\"\n",
    "        if not self.load_raw_text(filepath):\n",
    "            return {}, [], []\n",
    "\n",
    "        lines = self.raw_text.strip().split('\\n')\n",
    "        current_folio = None\n",
    "        current_folio_key = None\n",
    "        folio_pattern = r'<f(\\d+)([rv])?\\.'\n",
    "        all_words = []\n",
    "        sentences = []\n",
    "        current_sentence = []\n",
    "\n",
    "        def replace_uncertain(match):\n",
    "            options = match.group(1).split(':')\n",
    "            return options[0] if options else ''\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "                continue\n",
    "\n",
    "            cleaned_line = line\n",
    "            cleaned_line = re.sub(r'@\\d+', '', cleaned_line)\n",
    "            cleaned_line = re.sub(r'<![^>]*>', '', cleaned_line)\n",
    "            cleaned_line = re.sub(r'<[^>]*>', '', cleaned_line)\n",
    "            cleaned_line = re.sub(r'\\[([^\\]]+)\\]', replace_uncertain, cleaned_line)\n",
    "            cleaned_line = re.sub(r'[{}]', '', cleaned_line)\n",
    "            cleaned_line = re.sub(r'\\?+', '', cleaned_line)\n",
    "            cleaned_line = re.sub(r'[^a-zA-Z\\s,.]', '', cleaned_line)\n",
    "            if treat_commas_as_spaces:\n",
    "                cleaned_line = cleaned_line.replace('.', ' ').replace(',', ' ')\n",
    "            else:\n",
    "                cleaned_line = cleaned_line.replace('.', ' ').replace(',', '')\n",
    "            cleaned_line = re.sub(r'\\s+', ' ', cleaned_line).strip().lower()\n",
    "\n",
    "            folio_match = re.search(folio_pattern, line)\n",
    "            if folio_match:\n",
    "                current_folio = int(folio_match.group(1))\n",
    "                folio_side = folio_match.group(2) or 'r'\n",
    "                current_folio_key = f\"{current_folio}{folio_side}\"\n",
    "                folio_text = cleaned_line\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                folio_text = cleaned_line\n",
    "\n",
    "            if current_folio and folio_text:\n",
    "                if current_folio_key not in self.folio_data:\n",
    "                    self.folio_data[current_folio_key] = {\n",
    "                        'folio_num': current_folio,\n",
    "                        'folio_side': folio_side,\n",
    "                        'raw_text': [],\n",
    "                        'clean_words': [],\n",
    "                        'clean_glyphs': []\n",
    "                    }\n",
    "                self.folio_data[current_folio_key]['raw_text'].append(folio_text)\n",
    "                if glyph_level:\n",
    "                    glyphs = list(folio_text)\n",
    "                    glyphs = [g for g in glyphs if g.isalpha()]\n",
    "                    self.folio_data[current_folio_key]['clean_glyphs'].extend(glyphs)\n",
    "                words = folio_text.split()\n",
    "                clean_words = [w for w in words if re.match(r'^[a-z]+$', w) and len(w) >= min_word_length]\n",
    "                self.folio_data[current_folio_key]['clean_words'].extend(clean_words)\n",
    "                all_words.extend(clean_words)\n",
    "                current_sentence.extend(clean_words)\n",
    "\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "        sentences = [s for s in sentences if len(s) > 1]\n",
    "        return self.folio_data, all_words, sentences\n",
    "\n",
    "    def levenshtein_distance(self, seq1: List[str], seq2: List[str], return_alignment: bool = False) -> Tuple[int, Any]:\n",
    "        \"\"\"Compute Levenshtein distance and optional alignment between two sequences.\"\"\"\n",
    "        m, n = len(seq1), len(seq2)\n",
    "        if m < n:\n",
    "            dist, align = self.levenshtein_distance(seq2, seq1, return_alignment)\n",
    "            if return_alignment:\n",
    "                flipped_align = [(b, a, {'insert': 'delete', 'delete': 'insert'}.get(op, op)) for a, b, op in reversed(align)]\n",
    "                return dist, flipped_align\n",
    "            return dist, None\n",
    "\n",
    "        if n == 0:\n",
    "            if return_alignment:\n",
    "                return m, [(c, '-', 'delete') for c in seq1]\n",
    "            return m, None\n",
    "\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        backpointer = [[None] * (n + 1) for _ in range(m + 1)] if return_alignment else None\n",
    "\n",
    "        for i in range(m + 1):\n",
    "            dp[i][0] = i\n",
    "            if return_alignment:\n",
    "                backpointer[i][0] = ('delete', i - 1, 0) if i > 0 else None\n",
    "\n",
    "        for j in range(n + 1):\n",
    "            dp[0][j] = j\n",
    "            if return_alignment:\n",
    "                backpointer[0][j] = ('insert', 0, j - 1) if j > 0 else None\n",
    "\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                cost = 0 if seq1[i-1] == seq2[j-1] else 1\n",
    "                deletion = dp[i-1][j] + 1\n",
    "                insertion = dp[i][j-1] + 1\n",
    "                substitution = dp[i-1][j-1] + cost\n",
    "                min_val = min(deletion, insertion, substitution)\n",
    "                dp[i][j] = min_val\n",
    "\n",
    "                if return_alignment:\n",
    "                    if min_val == substitution:\n",
    "                        op = 'match' if cost == 0 else 'substitute'\n",
    "                        backpointer[i][j] = (op, i-1, j-1)\n",
    "                    elif min_val == insertion:\n",
    "                        backpointer[i][j] = ('insert', i, j-1)\n",
    "                    else:\n",
    "                        backpointer[i][j] = ('delete', i-1, j)\n",
    "\n",
    "        distance = dp[m][n]\n",
    "        if not return_alignment:\n",
    "            return distance, None\n",
    "\n",
    "        alignment = []\n",
    "        i, j = m, n\n",
    "        while i > 0 or j > 0:\n",
    "            if backpointer[i][j] is None:\n",
    "                break\n",
    "            op, prev_i, prev_j = backpointer[i][j]\n",
    "            if op == 'insert':\n",
    "                alignment.append(('-', seq2[j-1], 'insert'))\n",
    "                i, j = prev_i, prev_j\n",
    "            elif op == 'delete':\n",
    "                alignment.append((seq1[i-1], '-', 'delete'))\n",
    "                i, j = prev_i, prev_j\n",
    "            else:\n",
    "                alignment.append((seq1[i-1], seq2[j-1], op))\n",
    "                i, j = prev_i, prev_j\n",
    "\n",
    "        alignment.reverse()\n",
    "        return distance, alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## Section I: Pre-preparation\n",
    "\n",
    "Load and parse the ZL transliteration file, tokenize words in folio 1r, and display a DataFrame of words and their tokens (e.g., 'fachys' → 'f, a, ch, y, s')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section I: Pre-preparation for folio 1r:\n",
      "        Word          Tokenized\n",
      "0     fachys     f, a, ch, y, s\n",
      "1       ykal         y, k, a, l\n",
      "2         ar               a, r\n",
      "3     ataiin   a, t, a, i, i, n\n",
      "4       shol           sh, o, l\n",
      "..       ...                ...\n",
      "202     chol           ch, o, l\n",
      "203     chok           ch, o, k\n",
      "204    choty        ch, o, t, y\n",
      "205   chotey     ch, o, t, e, y\n",
      "206  dchaiin  d, ch, a, i, i, n\n",
      "\n",
      "[207 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "processor = VoynichTextProcessor()\n",
    "filepath = \"transliteration_zl.txt\"\n",
    "folio_data, all_words, sentences = processor.parse_all_folios(filepath)\n",
    "\n",
    "folio_key = '1r'\n",
    "if folio_key not in folio_data or len(folio_data[folio_key]['clean_words']) < 2:\n",
    "    print(f\"Folio {folio_key} not found or insufficient words.\")\n",
    "else:\n",
    "    words = folio_data[folio_key]['clean_words']\n",
    "    tokenized_words = [processor.tokenize_word(word) for word in words]\n",
    "    df_preparation = pd.DataFrame({\n",
    "        'Word': words,\n",
    "        'Tokenized': [', '.join(tokens) for tokens in tokenized_words]\n",
    "    })\n",
    "    print(f\"Section I: Pre-preparation for folio {folio_key}:\")\n",
    "    print(df_preparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## Section II: All Pairwise Distances\n",
    "\n",
    "Compute Levenshtein distances for all unique word pairs in folio 1r and display the results with the average distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "all_pairs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section II: All pairwise distances for folio 1r:\n",
      "        Word1    Word2  Distance\n",
      "0      fachys     ykal         5\n",
      "1      fachys       ar         4\n",
      "2      fachys   ataiin         5\n",
      "3      fachys     shol         5\n",
      "4      fachys    shory         4\n",
      "...       ...      ...       ...\n",
      "21316    chok   chotey         3\n",
      "21317    chok  dchaiin         5\n",
      "21318   choty   chotey         1\n",
      "21319   choty  dchaiin         5\n",
      "21320  chotey  dchaiin         5\n",
      "\n",
      "[21321 rows x 3 columns]\n",
      "Average Levenshtein distance (all pairs): 4.314009661835748\n"
     ]
    }
   ],
   "source": [
    "if folio_key not in folio_data or len(folio_data[folio_key]['clean_words']) < 2:\n",
    "    print(f\"Folio {folio_key} not found or insufficient words.\")\n",
    "else:\n",
    "    pairs = []\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    for i in range(len(tokenized_words)):\n",
    "        for j in range(i + 1, len(tokenized_words)):\n",
    "            dist, _ = processor.levenshtein_distance(tokenized_words[i], tokenized_words[j])\n",
    "            pairs.append({\n",
    "                'Word1': words[i],\n",
    "                'Word2': words[j],\n",
    "                'Distance': dist\n",
    "            })\n",
    "            total_distance += dist\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        df_all_pairs = pd.DataFrame(pairs)\n",
    "        print(f\"Section II: All pairwise distances for folio {folio_key}:\")\n",
    "        print(df_all_pairs)\n",
    "        average_distance = total_distance / count\n",
    "        print(f\"Average Levenshtein distance (all pairs): {average_distance}\")\n",
    "    else:\n",
    "        print(\"No pairs to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## Section III: Closest Words with Alignment Details\n",
    "\n",
    "For each word in folio 1r, find the closest non-identical word, compute Levenshtein alignments (match/substitute/insert/delete operations), and show operation counts and glyph differences. Filter for distances ≥1, sort by distance, and compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "closest",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minimum Levenshtein distance (excluding identical words): 1.3214285714285714\n",
      "Number of word pairs with distance >= 1: 140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>MostSimilar</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Alignment</th>\n",
       "      <th>Operations</th>\n",
       "      <th>Num_Substitutions</th>\n",
       "      <th>Num_Insertions</th>\n",
       "      <th>Num_Deletions</th>\n",
       "      <th>Num_Matches</th>\n",
       "      <th>Difference_Glyphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oiin</td>\n",
       "      <td>soiin</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\"n\", \"n\", \"match\"], [\"i\", \"i\", \"match\"], [\"i...</td>\n",
       "      <td>[\"match\", \"match\", \"match\", \"match\", \"insert\"]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-ins-&gt;s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dchar</td>\n",
       "      <td>char</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\"d\", \"-\", \"delete\"], [\"ch\", \"ch\", \"match\"], ...</td>\n",
       "      <td>[\"delete\", \"match\", \"match\", \"match\"]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>d-del-&gt;-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ain</td>\n",
       "      <td>dain</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\"n\", \"n\", \"match\"], [\"i\", \"i\", \"match\"], [\"a...</td>\n",
       "      <td>[\"match\", \"match\", \"match\", \"insert\"]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-ins-&gt;d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kodshey</td>\n",
       "      <td>koshey</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\"k\", \"k\", \"match\"], [\"o\", \"o\", \"match\"], [\"d...</td>\n",
       "      <td>[\"match\", \"match\", \"delete\", \"match\", \"match\",...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>d-del-&gt;-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she</td>\n",
       "      <td>sh</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\"sh\", \"sh\", \"match\"], [\"e\", \"-\", \"delete\"]]</td>\n",
       "      <td>[\"match\", \"delete\"]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>e-del-&gt;-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>ckhyds</td>\n",
       "      <td>cthres</td>\n",
       "      <td>3</td>\n",
       "      <td>[[\"c\", \"c\", \"match\"], [\"k\", \"t\", \"substitute\"]...</td>\n",
       "      <td>[\"match\", \"substitute\", \"match\", \"substitute\",...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>k-&gt;t; y-&gt;r; d-&gt;e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>ydaraishy</td>\n",
       "      <td>daraiin</td>\n",
       "      <td>3</td>\n",
       "      <td>[[\"y\", \"-\", \"delete\"], [\"d\", \"d\", \"match\"], [\"...</td>\n",
       "      <td>[\"delete\", \"match\", \"match\", \"match\", \"match\",...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>y-del-&gt;-; sh-&gt;i; y-&gt;n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>ctholdar</td>\n",
       "      <td>cthar</td>\n",
       "      <td>3</td>\n",
       "      <td>[[\"c\", \"c\", \"match\"], [\"t\", \"t\", \"match\"], [\"h...</td>\n",
       "      <td>[\"match\", \"match\", \"match\", \"delete\", \"delete\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>o-del-&gt;-; l-del-&gt;-; d-del-&gt;-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>fachys</td>\n",
       "      <td>chy</td>\n",
       "      <td>3</td>\n",
       "      <td>[[\"f\", \"-\", \"delete\"], [\"a\", \"-\", \"delete\"], [...</td>\n",
       "      <td>[\"delete\", \"delete\", \"match\", \"match\", \"delete\"]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>f-del-&gt;-; a-del-&gt;-; s-del-&gt;-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>tshodeesy</td>\n",
       "      <td>oteey</td>\n",
       "      <td>4</td>\n",
       "      <td>[[\"t\", \"-\", \"delete\"], [\"sh\", \"-\", \"delete\"], ...</td>\n",
       "      <td>[\"delete\", \"delete\", \"match\", \"substitute\", \"m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>t-del-&gt;-; sh-del-&gt;-; d-&gt;t; s-del-&gt;-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word MostSimilar  Distance  \\\n",
       "0         oiin       soiin         1   \n",
       "1        dchar        char         1   \n",
       "2          ain        dain         1   \n",
       "3      kodshey      koshey         1   \n",
       "4          she          sh         1   \n",
       "..         ...         ...       ...   \n",
       "135     ckhyds      cthres         3   \n",
       "136  ydaraishy     daraiin         3   \n",
       "137   ctholdar       cthar         3   \n",
       "138     fachys         chy         3   \n",
       "139  tshodeesy       oteey         4   \n",
       "\n",
       "                                             Alignment  \\\n",
       "0    [[\"n\", \"n\", \"match\"], [\"i\", \"i\", \"match\"], [\"i...   \n",
       "1    [[\"d\", \"-\", \"delete\"], [\"ch\", \"ch\", \"match\"], ...   \n",
       "2    [[\"n\", \"n\", \"match\"], [\"i\", \"i\", \"match\"], [\"a...   \n",
       "3    [[\"k\", \"k\", \"match\"], [\"o\", \"o\", \"match\"], [\"d...   \n",
       "4        [[\"sh\", \"sh\", \"match\"], [\"e\", \"-\", \"delete\"]]   \n",
       "..                                                 ...   \n",
       "135  [[\"c\", \"c\", \"match\"], [\"k\", \"t\", \"substitute\"]...   \n",
       "136  [[\"y\", \"-\", \"delete\"], [\"d\", \"d\", \"match\"], [\"...   \n",
       "137  [[\"c\", \"c\", \"match\"], [\"t\", \"t\", \"match\"], [\"h...   \n",
       "138  [[\"f\", \"-\", \"delete\"], [\"a\", \"-\", \"delete\"], [...   \n",
       "139  [[\"t\", \"-\", \"delete\"], [\"sh\", \"-\", \"delete\"], ...   \n",
       "\n",
       "                                            Operations  Num_Substitutions  \\\n",
       "0       [\"match\", \"match\", \"match\", \"match\", \"insert\"]                  0   \n",
       "1                [\"delete\", \"match\", \"match\", \"match\"]                  0   \n",
       "2                [\"match\", \"match\", \"match\", \"insert\"]                  0   \n",
       "3    [\"match\", \"match\", \"delete\", \"match\", \"match\",...                  0   \n",
       "4                                  [\"match\", \"delete\"]                  0   \n",
       "..                                                 ...                ...   \n",
       "135  [\"match\", \"substitute\", \"match\", \"substitute\",...                  3   \n",
       "136  [\"delete\", \"match\", \"match\", \"match\", \"match\",...                  2   \n",
       "137  [\"match\", \"match\", \"match\", \"delete\", \"delete\"...                  0   \n",
       "138   [\"delete\", \"delete\", \"match\", \"match\", \"delete\"]                  0   \n",
       "139  [\"delete\", \"delete\", \"match\", \"substitute\", \"m...                  1   \n",
       "\n",
       "     Num_Insertions  Num_Deletions  Num_Matches  \\\n",
       "0                 1              0            4   \n",
       "1                 0              1            3   \n",
       "2                 1              0            3   \n",
       "3                 0              1            5   \n",
       "4                 0              1            1   \n",
       "..              ...            ...          ...   \n",
       "135               0              0            3   \n",
       "136               0              1            5   \n",
       "137               0              3            5   \n",
       "138               0              3            2   \n",
       "139               0              3            4   \n",
       "\n",
       "                       Difference_Glyphs  \n",
       "0                                -ins->s  \n",
       "1                               d-del->-  \n",
       "2                                -ins->d  \n",
       "3                               d-del->-  \n",
       "4                               e-del->-  \n",
       "..                                   ...  \n",
       "135                     k->t; y->r; d->e  \n",
       "136                y-del->-; sh->i; y->n  \n",
       "137         o-del->-; l-del->-; d-del->-  \n",
       "138         f-del->-; a-del->-; s-del->-  \n",
       "139  t-del->-; sh-del->-; d->t; s-del->-  \n",
       "\n",
       "[140 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if folio_key not in folio_data or len(folio_data[folio_key]['clean_words']) < 2:\n",
    "    print(f\"Folio {folio_key} not found or insufficient words.\")\n",
    "    df_result = pd.DataFrame()\n",
    "else:\n",
    "    closest_pairs_data = []\n",
    "    total_min_distance = 0\n",
    "    for i in range(len(tokenized_words)):\n",
    "        min_dist = math.inf\n",
    "        closest_j = None\n",
    "        closest_alignment = None\n",
    "        for j in range(len(tokenized_words)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            dist, alignment = processor.levenshtein_distance(tokenized_words[i], tokenized_words[j], return_alignment=True)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_j = j\n",
    "                closest_alignment = alignment\n",
    "        if closest_j is not None:\n",
    "            ops = [op for _, _, op in closest_alignment]\n",
    "            num_subs = ops.count('substitute')\n",
    "            num_ins = ops.count('insert')\n",
    "            num_del = ops.count('delete')\n",
    "            num_match = ops.count('match')\n",
    "            diff_glyphs = []\n",
    "            for a, b, op in closest_alignment:\n",
    "                if op != 'match':\n",
    "                    if op == 'substitute':\n",
    "                        diff_glyphs.append(f\"{a}->{b}\")\n",
    "                    elif op == 'insert':\n",
    "                        diff_glyphs.append(f\"-ins->{b}\")\n",
    "                    elif op == 'delete':\n",
    "                        diff_glyphs.append(f\"{a}-del->-\")\n",
    "            closest_pairs_data.append({\n",
    "                'Word': words[i],\n",
    "                'MostSimilar': words[closest_j],\n",
    "                'Distance': min_dist,\n",
    "                'Alignment': json.dumps(closest_alignment),\n",
    "                'Operations': json.dumps(ops),\n",
    "                'Num_Substitutions': num_subs,\n",
    "                'Num_Insertions': num_ins,\n",
    "                'Num_Deletions': num_del,\n",
    "                'Num_Matches': num_match,\n",
    "                'Difference_Glyphs': '; '.join(diff_glyphs) if diff_glyphs else ''\n",
    "            })\n",
    "            total_min_distance += min_dist\n",
    "\n",
    "    df_result = pd.DataFrame(closest_pairs_data)\n",
    "    df_result = df_result[df_result['Distance'] >= 1].sort_values('Distance').reset_index(drop=True)\n",
    "    if len(df_result) > 0:\n",
    "        average_min_distance = df_result['Distance'].mean()\n",
    "        print(f\"Average minimum Levenshtein distance (excluding identical words): {average_min_distance}\")\n",
    "        print(f\"Number of word pairs with distance >= 1: {len(df_result)}\")\n",
    "    else:\n",
    "        print(\"No word pairs found with distance >= 1\")\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## Section IV: Distance to Predecessor\n",
    "\n",
    "Compute the Levenshtein distance between each word and its predecessor in folio 1r's sequence, showing the results and average distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "predecessor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section IV: Distance to predecessor for each word in folio 1r:\n",
      "     Index     Word Predecessor  Distance\n",
      "0        1     ykal      fachys         5\n",
      "1        2       ar        ykal         3\n",
      "2        3   ataiin          ar         5\n",
      "3        4     shol      ataiin         6\n",
      "4        5    shory        shol         2\n",
      "..     ...      ...         ...       ...\n",
      "201    202     chol          eo         2\n",
      "202    203     chok        chol         1\n",
      "203    204    choty        chok         2\n",
      "204    205   chotey       choty         1\n",
      "205    206  dchaiin      chotey         5\n",
      "\n",
      "[206 rows x 4 columns]\n",
      "Average Levenshtein distance to predecessor: 4.286407766990291\n"
     ]
    }
   ],
   "source": [
    "if folio_key not in folio_data or len(folio_data[folio_key]['clean_words']) < 2:\n",
    "    print(f\"Folio {folio_key} not found or insufficient words.\")\n",
    "else:\n",
    "    pred_pairs = []\n",
    "    total_pred_distance = 0\n",
    "    for i in range(1, len(tokenized_words)):\n",
    "        dist, _ = processor.levenshtein_distance(tokenized_words[i], tokenized_words[i-1])\n",
    "        pred_pairs.append({\n",
    "            'Index': i,\n",
    "            'Word': words[i],\n",
    "            'Predecessor': words[i-1],\n",
    "            'Distance': dist\n",
    "        })\n",
    "        total_pred_distance += dist\n",
    "\n",
    "    if pred_pairs:\n",
    "        df_pred = pd.DataFrame(pred_pairs)\n",
    "        print(f\"Section IV: Distance to predecessor for each word in folio {folio_key}:\")\n",
    "        print(df_pred)\n",
    "        average_pred_distance = total_pred_distance / len(pred_pairs)\n",
    "        print(f\"Average Levenshtein distance to predecessor: {average_pred_distance}\")\n",
    "    else:\n",
    "        print(\"No predecessor comparisons possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0481396-bb3a-403f-a0c0-ebcf721db05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of Word Creation for Folio 1r:\n",
      "          Word MostSimilar  Distance  \\\n",
      "0       fachys         chy         3   \n",
      "1          chy         chy         0   \n",
      "2          chy          sy         1   \n",
      "3           sy          cy         1   \n",
      "4           cy          ar         2   \n",
      "..         ...         ...       ...   \n",
      "201    daictoy   ydaraishy         5   \n",
      "202  ydaraishy   shcthaiin         6   \n",
      "203  shcthaiin    ctholdar         6   \n",
      "204   ctholdar   tshodeesy         7   \n",
      "205  tshodeesy   shokcheey         4   \n",
      "\n",
      "                                             Alignment  \\\n",
      "0    [[\"f\", \"-\", \"delete\"], [\"a\", \"-\", \"delete\"], [...   \n",
      "1         [[\"ch\", \"ch\", \"match\"], [\"y\", \"y\", \"match\"]]   \n",
      "2     [[\"ch\", \"s\", \"substitute\"], [\"y\", \"y\", \"match\"]]   \n",
      "3      [[\"s\", \"c\", \"substitute\"], [\"y\", \"y\", \"match\"]]   \n",
      "4    [[\"c\", \"a\", \"substitute\"], [\"y\", \"r\", \"substit...   \n",
      "..                                                 ...   \n",
      "201  [[\"y\", \"y\", \"match\"], [\"o\", \"sh\", \"substitute\"...   \n",
      "202  [[\"y\", \"sh\", \"substitute\"], [\"d\", \"c\", \"substi...   \n",
      "203  [[\"sh\", \"-\", \"delete\"], [\"c\", \"c\", \"match\"], [...   \n",
      "204  [[\"c\", \"-\", \"delete\"], [\"t\", \"t\", \"match\"], [\"...   \n",
      "205  [[\"t\", \"-\", \"delete\"], [\"sh\", \"sh\", \"match\"], ...   \n",
      "\n",
      "                                            Operations  Num_Substitutions  \\\n",
      "0     [\"delete\", \"delete\", \"match\", \"match\", \"delete\"]                  0   \n",
      "1                                   [\"match\", \"match\"]                  0   \n",
      "2                              [\"substitute\", \"match\"]                  1   \n",
      "3                              [\"substitute\", \"match\"]                  1   \n",
      "4                         [\"substitute\", \"substitute\"]                  2   \n",
      "..                                                 ...                ...   \n",
      "201  [\"match\", \"substitute\", \"substitute\", \"substit...                  4   \n",
      "202  [\"substitute\", \"substitute\", \"substitute\", \"su...                  6   \n",
      "203  [\"delete\", \"match\", \"match\", \"match\", \"insert\"...                  4   \n",
      "204  [\"delete\", \"match\", \"substitute\", \"match\", \"in...                  5   \n",
      "205  [\"delete\", \"match\", \"match\", \"substitute\", \"su...                  3   \n",
      "\n",
      "     Num_Insertions  Num_Deletions  Num_Matches  \\\n",
      "0                 0              3            2   \n",
      "1                 0              0            2   \n",
      "2                 0              0            1   \n",
      "3                 0              0            1   \n",
      "4                 0              0            0   \n",
      "..              ...            ...          ...   \n",
      "201               1              0            3   \n",
      "202               0              0            2   \n",
      "203               1              1            3   \n",
      "204               1              1            2   \n",
      "205               0              1            4   \n",
      "\n",
      "                                    Difference_Glyphs  \n",
      "0                        f-del->-; a-del->-; s-del->-  \n",
      "1                                                      \n",
      "2                                               ch->s  \n",
      "3                                                s->c  \n",
      "4                                          c->a; y->r  \n",
      "..                                                ...  \n",
      "201                  o->sh; t->i; c->a; i->r; -ins->y  \n",
      "202              y->sh; d->c; a->t; r->h; sh->i; y->n  \n",
      "203        sh-del->-; -ins->o; a->l; i->d; i->a; n->r  \n",
      "204  c-del->-; h->sh; -ins->d; l->e; d->e; a->s; r->y  \n",
      "205                       t-del->-; d->k; e->ch; s->e  \n",
      "\n",
      "[206 rows x 10 columns]\n",
      "\n",
      "Chain Coverage: 206/207 words\n",
      "Average Levenshtein Distance in Chain: 1.44\n",
      "\n",
      "Operation Summary in Chain:\n",
      "   Num_Substitutions  Num_Insertions  Num_Deletions  Num_Matches\n",
      "0                183              58             56          650\n",
      "\n",
      "Insight: High coverage with low distances and frequent ins/del suggest a systematic word derivation process.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Section VI: Chain of Word Creation Analysis\n",
    "# Tests the hypothesis that words in folio 1r form a chain where each word is derived\n",
    "# from the previous via minimal Levenshtein edits, starting from the first word.\n",
    "\n",
    "if 'folio_data' not in locals() or 'words' not in locals() or 'tokenized_words' not in locals() or 'processor' not in locals():\n",
    "    print(\"Required variables (folio_data, words, tokenized_words, processor) missing. Run Section I first.\")\n",
    "elif folio_key not in folio_data or len(folio_data[folio_key]['clean_words']) < 2:\n",
    "    print(f\"Folio {folio_key} not found or insufficient words.\")\n",
    "else:\n",
    "    # Initialize chain starting with the first word\n",
    "    chain = []\n",
    "    visited_indices = set()\n",
    "    current_idx = 0  # Start with first word\n",
    "    total_distance = 0\n",
    "    visited_indices.add(current_idx)\n",
    "\n",
    "    while len(visited_indices) < len(words):\n",
    "        current_word = words[current_idx]\n",
    "        current_tokens = tokenized_words[current_idx]\n",
    "        min_dist = math.inf\n",
    "        next_idx = None\n",
    "        next_alignment = None\n",
    "\n",
    "        # Find unvisited word with minimum Levenshtein distance\n",
    "        for j in range(len(tokenized_words)):\n",
    "            if j in visited_indices:\n",
    "                continue\n",
    "            dist, alignment = processor.levenshtein_distance(current_tokens, tokenized_words[j], return_alignment=True)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                next_idx = j\n",
    "                next_alignment = alignment\n",
    "\n",
    "        if next_idx is None:\n",
    "            break  # No more unvisited words\n",
    "\n",
    "        # Extract operation details\n",
    "        ops = [op for _, _, op in next_alignment]\n",
    "        num_subs = ops.count('substitute')\n",
    "        num_ins = ops.count('insert')\n",
    "        num_del = ops.count('delete')\n",
    "        num_match = ops.count('match')\n",
    "        diff_glyphs = []\n",
    "        for a, b, op in next_alignment:\n",
    "            if op != 'match':\n",
    "                if op == 'substitute':\n",
    "                    diff_glyphs.append(f\"{a}->{b}\")\n",
    "                elif op == 'insert':\n",
    "                    diff_glyphs.append(f\"-ins->{b}\")\n",
    "                elif op == 'delete':\n",
    "                    diff_glyphs.append(f\"{a}-del->-\")\n",
    "\n",
    "        chain.append({\n",
    "            'Word': current_word,\n",
    "            'MostSimilar': words[next_idx],\n",
    "            'Distance': min_dist,\n",
    "            'Alignment': json.dumps(next_alignment),\n",
    "            'Operations': json.dumps(ops),\n",
    "            'Num_Substitutions': num_subs,\n",
    "            'Num_Insertions': num_ins,\n",
    "            'Num_Deletions': num_del,\n",
    "            'Num_Matches': num_match,\n",
    "            'Difference_Glyphs': '; '.join(diff_glyphs) if diff_glyphs else ''\n",
    "        })\n",
    "\n",
    "        total_distance += min_dist\n",
    "        visited_indices.add(next_idx)\n",
    "        current_idx = next_idx\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_chain = pd.DataFrame(chain)\n",
    "    print(f\"Chain of Word Creation for Folio {folio_key}:\")\n",
    "    print(df_chain)\n",
    "    print(f\"\\nChain Coverage: {len(chain)}/{len(words)} words\")\n",
    "    if len(chain) > 0:\n",
    "        avg_distance = total_distance / len(chain)\n",
    "        print(f\"Average Levenshtein Distance in Chain: {avg_distance:.2f}\")\n",
    "    else:\n",
    "        print(\"No chain formed.\")\n",
    "\n",
    "    # Summary of operation types\n",
    "    if not df_chain.empty:\n",
    "        op_summary = df_chain[['Num_Substitutions', 'Num_Insertions', 'Num_Deletions', 'Num_Matches']].sum()\n",
    "        print(\"\\nOperation Summary in Chain:\")\n",
    "        print(op_summary.to_frame().T)\n",
    "        print(\"\\nInsight: High coverage with low distances and frequent ins/del suggest a systematic word derivation process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3036c35-9c13-4dab-9923-b5ac86c2d10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>MostSimilar</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Alignment</th>\n",
       "      <th>Operations</th>\n",
       "      <th>Num_Substitutions</th>\n",
       "      <th>Num_Insertions</th>\n",
       "      <th>Num_Deletions</th>\n",
       "      <th>Num_Matches</th>\n",
       "      <th>Difference_Glyphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fachys</td>\n",
       "      <td>chy</td>\n",
       "      <td>3</td>\n",
       "      <td>[[\"f\", \"-\", \"delete\"], [\"a\", \"-\", \"delete\"], [...</td>\n",
       "      <td>[\"delete\", \"delete\", \"match\", \"match\", \"delete\"]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>f-del-&gt;-; a-del-&gt;-; s-del-&gt;-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chy</td>\n",
       "      <td>chy</td>\n",
       "      <td>0</td>\n",
       "      <td>[[\"ch\", \"ch\", \"match\"], [\"y\", \"y\", \"match\"]]</td>\n",
       "      <td>[\"match\", \"match\"]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chy</td>\n",
       "      <td>sy</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\"ch\", \"s\", \"substitute\"], [\"y\", \"y\", \"match\"]]</td>\n",
       "      <td>[\"substitute\", \"match\"]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ch-&gt;s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sy</td>\n",
       "      <td>cy</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\"s\", \"c\", \"substitute\"], [\"y\", \"y\", \"match\"]]</td>\n",
       "      <td>[\"substitute\", \"match\"]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>s-&gt;c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cy</td>\n",
       "      <td>ar</td>\n",
       "      <td>2</td>\n",
       "      <td>[[\"c\", \"a\", \"substitute\"], [\"y\", \"r\", \"substit...</td>\n",
       "      <td>[\"substitute\", \"substitute\"]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c-&gt;a; y-&gt;r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>daictoy</td>\n",
       "      <td>ydaraishy</td>\n",
       "      <td>5</td>\n",
       "      <td>[[\"y\", \"y\", \"match\"], [\"o\", \"sh\", \"substitute\"...</td>\n",
       "      <td>[\"match\", \"substitute\", \"substitute\", \"substit...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>o-&gt;sh; t-&gt;i; c-&gt;a; i-&gt;r; -ins-&gt;y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>ydaraishy</td>\n",
       "      <td>shcthaiin</td>\n",
       "      <td>6</td>\n",
       "      <td>[[\"y\", \"sh\", \"substitute\"], [\"d\", \"c\", \"substi...</td>\n",
       "      <td>[\"substitute\", \"substitute\", \"substitute\", \"su...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>y-&gt;sh; d-&gt;c; a-&gt;t; r-&gt;h; sh-&gt;i; y-&gt;n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>shcthaiin</td>\n",
       "      <td>ctholdar</td>\n",
       "      <td>6</td>\n",
       "      <td>[[\"sh\", \"-\", \"delete\"], [\"c\", \"c\", \"match\"], [...</td>\n",
       "      <td>[\"delete\", \"match\", \"match\", \"match\", \"insert\"...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>sh-del-&gt;-; -ins-&gt;o; a-&gt;l; i-&gt;d; i-&gt;a; n-&gt;r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>ctholdar</td>\n",
       "      <td>tshodeesy</td>\n",
       "      <td>7</td>\n",
       "      <td>[[\"c\", \"-\", \"delete\"], [\"t\", \"t\", \"match\"], [\"...</td>\n",
       "      <td>[\"delete\", \"match\", \"substitute\", \"match\", \"in...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>c-del-&gt;-; h-&gt;sh; -ins-&gt;d; l-&gt;e; d-&gt;e; a-&gt;s; r-&gt;y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>tshodeesy</td>\n",
       "      <td>shokcheey</td>\n",
       "      <td>4</td>\n",
       "      <td>[[\"t\", \"-\", \"delete\"], [\"sh\", \"sh\", \"match\"], ...</td>\n",
       "      <td>[\"delete\", \"match\", \"match\", \"substitute\", \"su...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>t-del-&gt;-; d-&gt;k; e-&gt;ch; s-&gt;e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word MostSimilar  Distance  \\\n",
       "0       fachys         chy         3   \n",
       "1          chy         chy         0   \n",
       "2          chy          sy         1   \n",
       "3           sy          cy         1   \n",
       "4           cy          ar         2   \n",
       "..         ...         ...       ...   \n",
       "201    daictoy   ydaraishy         5   \n",
       "202  ydaraishy   shcthaiin         6   \n",
       "203  shcthaiin    ctholdar         6   \n",
       "204   ctholdar   tshodeesy         7   \n",
       "205  tshodeesy   shokcheey         4   \n",
       "\n",
       "                                             Alignment  \\\n",
       "0    [[\"f\", \"-\", \"delete\"], [\"a\", \"-\", \"delete\"], [...   \n",
       "1         [[\"ch\", \"ch\", \"match\"], [\"y\", \"y\", \"match\"]]   \n",
       "2     [[\"ch\", \"s\", \"substitute\"], [\"y\", \"y\", \"match\"]]   \n",
       "3      [[\"s\", \"c\", \"substitute\"], [\"y\", \"y\", \"match\"]]   \n",
       "4    [[\"c\", \"a\", \"substitute\"], [\"y\", \"r\", \"substit...   \n",
       "..                                                 ...   \n",
       "201  [[\"y\", \"y\", \"match\"], [\"o\", \"sh\", \"substitute\"...   \n",
       "202  [[\"y\", \"sh\", \"substitute\"], [\"d\", \"c\", \"substi...   \n",
       "203  [[\"sh\", \"-\", \"delete\"], [\"c\", \"c\", \"match\"], [...   \n",
       "204  [[\"c\", \"-\", \"delete\"], [\"t\", \"t\", \"match\"], [\"...   \n",
       "205  [[\"t\", \"-\", \"delete\"], [\"sh\", \"sh\", \"match\"], ...   \n",
       "\n",
       "                                            Operations  Num_Substitutions  \\\n",
       "0     [\"delete\", \"delete\", \"match\", \"match\", \"delete\"]                  0   \n",
       "1                                   [\"match\", \"match\"]                  0   \n",
       "2                              [\"substitute\", \"match\"]                  1   \n",
       "3                              [\"substitute\", \"match\"]                  1   \n",
       "4                         [\"substitute\", \"substitute\"]                  2   \n",
       "..                                                 ...                ...   \n",
       "201  [\"match\", \"substitute\", \"substitute\", \"substit...                  4   \n",
       "202  [\"substitute\", \"substitute\", \"substitute\", \"su...                  6   \n",
       "203  [\"delete\", \"match\", \"match\", \"match\", \"insert\"...                  4   \n",
       "204  [\"delete\", \"match\", \"substitute\", \"match\", \"in...                  5   \n",
       "205  [\"delete\", \"match\", \"match\", \"substitute\", \"su...                  3   \n",
       "\n",
       "     Num_Insertions  Num_Deletions  Num_Matches  \\\n",
       "0                 0              3            2   \n",
       "1                 0              0            2   \n",
       "2                 0              0            1   \n",
       "3                 0              0            1   \n",
       "4                 0              0            0   \n",
       "..              ...            ...          ...   \n",
       "201               1              0            3   \n",
       "202               0              0            2   \n",
       "203               1              1            3   \n",
       "204               1              1            2   \n",
       "205               0              1            4   \n",
       "\n",
       "                                    Difference_Glyphs  \n",
       "0                        f-del->-; a-del->-; s-del->-  \n",
       "1                                                      \n",
       "2                                               ch->s  \n",
       "3                                                s->c  \n",
       "4                                          c->a; y->r  \n",
       "..                                                ...  \n",
       "201                  o->sh; t->i; c->a; i->r; -ins->y  \n",
       "202              y->sh; d->c; a->t; r->h; sh->i; y->n  \n",
       "203        sh-del->-; -ins->o; a->l; i->d; i->a; n->r  \n",
       "204  c-del->-; h->sh; -ins->d; l->e; d->e; a->s; r->y  \n",
       "205                       t-del->-; d->k; e->ch; s->e  \n",
       "\n",
       "[206 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
