{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf7fbc9-5f04-4a58-aabe-2f63d2b91793",
   "metadata": {},
   "source": [
    "### Phonotatic Constraits Analysis\n",
    "\n",
    "Atempt of morphological decipherement (via phonotatic constraits) based on the hypothesis of Naibbe Cipher: https://youtu.be/ByARtG-GUPo?t=5559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5ba215-a7b7-4093-9cd8-5fb0af352069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folio</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1r</td>\n",
       "      <td>fachys ykal ar ataiin shol shory cthres kor sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1r</td>\n",
       "      <td>sory ckhar or kair chtaiin shar ase cthar ctha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1r</td>\n",
       "      <td>syaiir sheky or ykaiin shod cthoary cthes dara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1r</td>\n",
       "      <td>soiin oteey oteos roloty cthiar daiin okaiin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1r</td>\n",
       "      <td>sair chear cthaiin cphar cfhaiin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td>116r</td>\n",
       "      <td>osain shky qorain chckhey qokey lkechy okeey o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>116r</td>\n",
       "      <td>sykar ain olkeey dainchey qokar chey dain otan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5304</th>\n",
       "      <td>116r</td>\n",
       "      <td>sysor shey qokey okeolan chey qol or cheey qor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>116r</td>\n",
       "      <td>sodal ch al chcthy chckhy qol ain ary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>116v</td>\n",
       "      <td>oror sheey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5307 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     folio                                               text\n",
       "0       1r  fachys ykal ar ataiin shol shory cthres kor sh...\n",
       "1       1r  sory ckhar or kair chtaiin shar ase cthar ctha...\n",
       "2       1r  syaiir sheky or ykaiin shod cthoary cthes dara...\n",
       "3       1r  soiin oteey oteos roloty cthiar daiin okaiin o...\n",
       "4       1r                   sair chear cthaiin cphar cfhaiin\n",
       "...    ...                                                ...\n",
       "5302  116r  osain shky qorain chckhey qokey lkechy okeey o...\n",
       "5303  116r  sykar ain olkeey dainchey qokar chey dain otan...\n",
       "5304  116r  sysor shey qokey okeolan chey qol or cheey qor...\n",
       "5305  116r              sodal ch al chcthy chckhy qol ain ary\n",
       "5306  116v                                         oror sheey\n",
       "\n",
       "[5307 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class VoynichTextProcessor:\n",
    "    \"\"\"Processes Voynich Manuscript text to return a cleaned DataFrame.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw_text = None\n",
    "\n",
    "    def load_raw_text(self, filepath: str) -> bool:\n",
    "        \"\"\"Load raw text from file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                self.raw_text = f.read()\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    def clean_voynich_text(self, filepath: str, treat_commas_as_spaces: bool = True, min_word_length: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"Clean Voynich Manuscript text and return a DataFrame\"\"\"\n",
    "        if not self.load_raw_text(filepath):\n",
    "            return pd.DataFrame(columns=['folio', 'text'])\n",
    "\n",
    "        lines = self.raw_text.strip().split('\\n')\n",
    "        folio_pattern = r'<f(\\d+)([rv])?\\.'\n",
    "        data = []\n",
    "        current_folio = None\n",
    "\n",
    "        def replace_uncertain(match):\n",
    "            options = match.group(1).split(':')\n",
    "            return options[0] if options else ''\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "\n",
    "            cleaned_line = line\n",
    "            cleaned_line = re.sub(r'@\\d+', '', cleaned_line)  # Remove annotations\n",
    "            cleaned_line = re.sub(r'<![^>]*>', '', cleaned_line)  # Remove comments\n",
    "            cleaned_line = re.sub(r'<[^>]*>', '', cleaned_line)  # Remove other markup\n",
    "            cleaned_line = re.sub(r'\\[([^\\]]+)\\]', replace_uncertain, cleaned_line)  # Handle uncertain readings\n",
    "            cleaned_line = re.sub(r'[{}]', '', cleaned_line)  # Remove braces\n",
    "            cleaned_line = re.sub(r'\\?+', '', cleaned_line)  # Remove question marks\n",
    "            cleaned_line = re.sub(r'[^a-zA-Z\\s,.]', '', cleaned_line)  # Keep only letters, spaces, commas, periods\n",
    "            if treat_commas_as_spaces:\n",
    "                cleaned_line = cleaned_line.replace('.', ' ').replace(',', ' ')\n",
    "            else:\n",
    "                cleaned_line = cleaned_line.replace('.', ' ').replace(',', '')\n",
    "            cleaned_line = re.sub(r'\\s+', ' ', cleaned_line).strip().lower()  # Normalize spaces, lowercase\n",
    "\n",
    "            folio_match = re.search(folio_pattern, line)\n",
    "            if folio_match:\n",
    "                current_folio = f\"{folio_match.group(1)}{folio_match.group(2) or 'r'}\"\n",
    "            elif re.match(folio_pattern, cleaned_line):\n",
    "                continue\n",
    "\n",
    "            if cleaned_line and current_folio:\n",
    "                words = cleaned_line.split()\n",
    "                clean_words = [w for w in words if re.match(r'^[a-z]+$', w) and len(w) >= min_word_length]\n",
    "                if clean_words:\n",
    "                    data.append({'folio': current_folio, 'text': ' '.join(clean_words)})\n",
    "\n",
    "        text = pd.DataFrame(data, columns=['folio', 'text'])\n",
    "        return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = VoynichTextProcessor()\n",
    "    filepath = \"transliteration_zl.txt\"\n",
    "    text = processor.clean_voynich_text(filepath)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8e3af7-262f-4dbe-aee7-1efb6779d047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>sequence</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3</td>\n",
       "      <td>chol chol kor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>3</td>\n",
       "      <td>chol daiin cthy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>3</td>\n",
       "      <td>daiin cthy schey</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>3</td>\n",
       "      <td>chol chy chaiin</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>3</td>\n",
       "      <td>cthor chol chor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>shol daiin</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2</td>\n",
       "      <td>otol chol</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2</td>\n",
       "      <td>chor daiin</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2</td>\n",
       "      <td>daiin dain</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>or aiin</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n          sequence  count\n",
       "348  3     chol chol kor      2\n",
       "349  3   chol daiin cthy      2\n",
       "350  3  daiin cthy schey      2\n",
       "351  3   chol chy chaiin      2\n",
       "352  3   cthor chol chor      2\n",
       "..  ..               ...    ...\n",
       "84   2        shol daiin      7\n",
       "88   2         otol chol      7\n",
       "102  2        chor daiin      8\n",
       "161  2        daiin dain      9\n",
       "138  2           or aiin     13\n",
       "\n",
       "[332 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def get_folio_number(folio):\n",
    "    \"\"\"Extract numeric part of folio (e.g., '1r' -> 1).\"\"\"\n",
    "    match = re.match(r'(\\d+)', folio)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    \"\"\"Generate n-grams from a list of words.\"\"\"\n",
    "    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "def find_ngram_frequencies(text_df, max_n=10):\n",
    "    \"\"\"Return a DataFrame with longest n-grams (2 to max_n) with frequencies >1 for folios 1 to 60.\"\"\"\n",
    "    # Filter for folios 1 to 60\n",
    "    filtered_text = text_df[text_df['folio'].apply(get_folio_number) <= 55]\n",
    "    \n",
    "    # Concatenate all text into one list of words\n",
    "    all_words = ' '.join(filtered_text['text'].fillna('').tolist()).split()\n",
    "    \n",
    "    # Collect n-gram data\n",
    "    ngram_data = []\n",
    "    \n",
    "    # Generate and count n-grams for n=2 to max_n\n",
    "    for n in range(2, max_n + 1):\n",
    "        ngrams = generate_ngrams(all_words, n)\n",
    "        ngram_counts = Counter(ngrams)\n",
    "        # Only include n-grams with count > 1\n",
    "        for seq, count in ngram_counts.items():\n",
    "            if count > 1:\n",
    "                ngram_data.append({'n': n, 'sequence': seq, 'count': count})\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(ngram_data, columns=['n', 'sequence', 'count'])\n",
    "    \n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Filter out shorter n-grams contained in longer ones\n",
    "    longest_ngrams = []\n",
    "    for n in range(max_n, 1, -1):  # Start with longest n-grams\n",
    "        current_ngrams = df[df['n'] == n]\n",
    "        for _, row in current_ngrams.iterrows():\n",
    "            seq = row['sequence']\n",
    "            is_subsequence = False\n",
    "            # Check if this sequence is contained in any longer n-gram\n",
    "            for _, longer_row in df[df['n'] > n].iterrows():\n",
    "                if seq in longer_row['sequence']:\n",
    "                    is_subsequence = True\n",
    "                    break\n",
    "            if not is_subsequence:\n",
    "                longest_ngrams.append(row)\n",
    "    \n",
    "    # Create final DataFrame with only longest n-grams\n",
    "    result_df = pd.DataFrame(longest_ngrams, columns=['n', 'sequence', 'count'])\n",
    "    return result_df.sort_values(by=['n', 'count'], ascending=[False, True])\n",
    "\n",
    "ngram_df = find_ngram_frequencies(text, max_n=10)\n",
    "ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f1ec3f-3db9-47f8-864e-6585ca102c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transitions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.sort_values(by=[\u001b[33m'\u001b[39m\u001b[33mfrequency\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m'\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m])\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Run the function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m valid_sequences_df = map_valid_sequences(text, \u001b[43mtransitions_df\u001b[49m)\n\u001b[32m     53\u001b[39m valid_sequences_df\n",
      "\u001b[31mNameError\u001b[39m: name 'transitions_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "def map_valid_sequences(text_df, transitions_df, max_n=10):\n",
    "    def get_folio_number(folio):\n",
    "        match = re.match(r'(\\d+)', folio)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    filtered_text = text_df[text_df['folio'].apply(get_folio_number) <= 60]\n",
    "    \n",
    "    successor_dict = {row['symbol']: set(row['successors'].split(', ')) if row['successors'] else set() for _, row in transitions_df.iterrows()}\n",
    "    \n",
    "    sequence_folios = defaultdict(list)\n",
    "    sequence_counts = Counter()\n",
    "    \n",
    "    for _, row in filtered_text.iterrows():\n",
    "        folio = row['folio']\n",
    "        words = row['text'].split()\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "        for n in range(2, min(max_n + 1, len(words) + 1)):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                subsequence = words[i:i+n]\n",
    "                is_valid = True\n",
    "                for j in range(len(subsequence) - 1):\n",
    "                    current = subsequence[j]\n",
    "                    next_word = subsequence[j + 1]\n",
    "                    if current not in successor_dict or next_word not in successor_dict[current]:\n",
    "                        is_valid = False\n",
    "                        break\n",
    "                if is_valid:\n",
    "                    seq_str = ' '.join(subsequence)\n",
    "                    sequence_counts[seq_str] += 1\n",
    "                    if folio not in sequence_folios[seq_str]:\n",
    "                        sequence_folios[seq_str].append(folio)\n",
    "    \n",
    "    valid_sequences = []\n",
    "    for seq, folios in sequence_folios.items():\n",
    "        valid_sequences.append({\n",
    "            'folio': ', '.join(sorted(folios)),\n",
    "            'sequence': seq,\n",
    "            'length': len(seq.split()),\n",
    "            'frequency': sequence_counts[seq]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(valid_sequences)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    return df.sort_values(by=['frequency','length'], ascending=[False, False])\n",
    "\n",
    "# Run the function\n",
    "valid_sequences_df = map_valid_sequences(text, transitions_df)\n",
    "valid_sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bafd1d-ba09-41a7-ad38-7be9b47ae88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "def extract_symbol_bigrams(symbols: List[str]) -> Set[str]:\n",
    "    \"\"\"Extract unique bigrams of symbols from a list of symbols.\"\"\"\n",
    "    bigrams = {f\"{symbols[i]}-{symbols[i+1]}\" for i in range(len(symbols) - 1)}\n",
    "    return bigrams\n",
    "\n",
    "def get_absent_symbol_bigrams(symbols: List[str], observed_bigrams: Set[str]) -> Set[str]:\n",
    "    \"\"\"Get all possible symbol bigrams not observed in the data.\"\"\"\n",
    "    alphabet = set(symbols)\n",
    "    all_possible_bigrams = {f\"{a}-{b}\" for a in alphabet for b in alphabet}\n",
    "    return all_possible_bigrams - observed_bigrams\n",
    "\n",
    "def find_longest_symbol_bigram_chain(symbols: List[str]) -> int:\n",
    "    \"\"\"Find the longest chain of bigrams where each shares a second symbol with the next's first.\"\"\"\n",
    "    if len(symbols) < 2:\n",
    "        return 0\n",
    "    max_chain = 1  # Minimum is one bigram\n",
    "    current_chain = 1\n",
    "    for i in range(len(symbols) - 2):\n",
    "        if symbols[i + 1] == symbols[i + 2]:  # Check if next bigram starts with current bigram's second symbol\n",
    "            current_chain += 1\n",
    "            max_chain = max(max_chain, current_chain)\n",
    "        else:\n",
    "            current_chain = 1\n",
    "    return max_chain\n",
    "\n",
    "def generate_symbol_bigram_rule(symbols: List[str]) -> str:\n",
    "    \"\"\"Generate a rule based on the longest chain of symbol bigrams.\"\"\"\n",
    "    max_chain = find_longest_symbol_bigram_chain(symbols)\n",
    "    if max_chain == 0:\n",
    "        return \"\"\n",
    "    return f\"(L²){{{max_chain}}}\"\n",
    "\n",
    "def process_voynich_symbol_sequences(valid_sequences_df: pd.DataFrame) -> Tuple[Set[str], Set[str], str]:\n",
    "    \"\"\"\n",
    "    Process valid_sequences_df to extract symbol bigrams, absent bigrams, and a rule.\n",
    "    Returns (observed_bigrams, absent_bigrams, rule).\n",
    "    \"\"\"\n",
    "    # Concatenate all sequences into a list of symbols\n",
    "    all_symbols = []\n",
    "    for seq in valid_sequences_df[\"sequence\"]:\n",
    "        symbols = seq.split()  # Split each sequence into symbols (e.g., \"chol daiin\" -> [\"chol\", \"daiin\"])\n",
    "        all_symbols.extend(symbols)\n",
    "    \n",
    "    # Extract bigrams\n",
    "    observed_bigrams = extract_symbol_bigrams(all_symbols)\n",
    "    \n",
    "    # Get absent bigrams\n",
    "    absent_bigrams = get_absent_symbol_bigrams(all_symbols, observed_bigrams)\n",
    "    \n",
    "    # Generate rule\n",
    "    rule = generate_symbol_bigram_rule(all_symbols)\n",
    "    \n",
    "    return observed_bigrams, absent_bigrams, rule\n",
    "\n",
    "def main():\n",
    "    # Assume valid_sequences_df is available from map_valid_sequences\n",
    "    observed_bigrams, absent_bigrams, rule = process_voynich_symbol_sequences(valid_sequences_df)\n",
    "    \n",
    "    # Output results\n",
    "    print(\"Possible symbol bigram combinations:\")\n",
    "    for bigram in sorted(observed_bigrams):\n",
    "        print(bigram)\n",
    "    \n",
    "    print(\"\\nAbsent symbol bigram combinations:\")\n",
    "    for bigram in sorted(absent_bigrams):\n",
    "        print(bigram)\n",
    "    \n",
    "    print(f\"\\nGeneralized rule: {rule}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
