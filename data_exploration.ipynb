{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ed50a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Language Corpus Collection and Voynich Manuscript Analysis\n",
    "\n",
    "This notebook provides tools for collecting language corpora from Wikipedia and analyzing text using Stolfi's grammar rules, specifically tailored for the Voynich Manuscript. The code has been refactored to remove duplications, improve readability, and follow professional coding standards with comprehensive comments.\n",
    "\n",
    "## Overview\n",
    "- **Corpus Collection**: Collects text data for natural and constructed languages from Wikipedia and Bible sources (where applicable).\n",
    "- **Voynich Analysis**: Analyzes text using Zipf's Law, Shannon Entropy, Stolfi's grammar rules, focusing on word structure, density profiles, and compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac4e2ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-03T05:49:06.544Z"
    },
    "deletable": false,
    "editable": true,
    "run_control": {
     "frozen": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: wikipedia in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: numpy in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pandas in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.3.2)\n",
      "Requirement already satisfied: matplotlib in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (3.10.5)\n",
      "Requirement already satisfied: scipy in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (1.16.1)\n",
      "Requirement already satisfied: seaborn in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: nltk in f:\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (3.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\miniconda3\\lib\\site-packages (from wikipedia->-r requirements.txt (line 2)) (4.13.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in f:\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in f:\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in f:\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in f:\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in f:\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in f:\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in f:\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in f:\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.3)\n",
      "Requirement already satisfied: click in f:\\miniconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 8)) (8.2.1)\n",
      "Requirement already satisfied: joblib in f:\\miniconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\miniconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 8)) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in f:\\miniconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in f:\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\miniconda3\\lib\\site-packages (from beautifulsoup4->wikipedia->-r requirements.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in f:\\miniconda3\\lib\\site-packages (from beautifulsoup4->wikipedia->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: colorama in f:\\miniconda3\\lib\\site-packages (from click->nltk->-r requirements.txt (line 8)) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a2c710-64e1-415a-9e3d-b7bff9ac6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0665a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Collecting Data\n",
    "\n",
    "This language sample represents major language families and typological varieties essential for comparative linguistic analysis when deciphering unknown languages. The selection includes Indo-European languages (English, Spanish, German, Russian, French, Polish, Bulgarian, Greek, Albanian, Romanian, Irish, Latin, Hebrew), Uralic languages (Finnish, Hungarian), and constructed languages (Esperanto), along with a simplified English variant. \n",
    "\n",
    "This broad representation is crucial because unknown languages often share structural features, borrowed vocabulary, or historical connections with known language families. By comparing functional words, grammatical patterns, and phonological structures across this typologically diverse set, we can identify potential language family affiliations, detect borrowing patterns, and recognize universal linguistic tendencies. The inclusion of both ancient (Latin, Hebrew) and modern languages, along with geographically dispersed varieties, maximizes the likelihood of finding meaningful correspondences and helps eliminate false cognates, making this collection an invaluable reference framework for systematic language decipherment efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02628cb7-5034-4ae7-bb4e-3733001f7b0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status:\n",
      "  english: 78101 words\n",
      "  spanish: 39864 words\n",
      "  french: 81741 words\n",
      "  german: 40040 words\n",
      "  russian: 47249 words\n",
      "  turkish: 9031 words\n",
      "  finnish: 30922 words\n",
      "  hungarian: 16568 words\n",
      "  basque: 13262 words\n",
      "  polish: 42694 words\n",
      "  bulgarian: 6842 words\n",
      "  greek: 42508 words\n",
      "  albanian: 17461 words\n",
      "  romanian: 40155 words\n",
      "  irish: 7170 words\n",
      "  maltese: 11912 words\n",
      "  arabic: 57965 words\n",
      "  hebrew: 10364 words\n",
      "  latin: 6200 words\n",
      "  italian: missing\n",
      "  simple_english: 32277 words\n",
      "  esperanto: 22669 words\n",
      "  lingua_franca_nova: 10766 words\n",
      "Need to collect: 1 languages\n",
      "\n",
      "--- Collecting for Italian (it) ---\n",
      "    Got 'HIStory: Past, Present and Future - Book I' (5570 words). Total: 5570\n",
      "    Skipping 'History' (DisambiguationError).\n",
      "    Skipping 'Culture' (DisambiguationError).\n",
      "    Got 'Cancel culture' (2345 words). Total: 7915\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "class UniversalCorpusCollector:\n",
    "    def __init__(self, natural_corpus_dir=\"language_corpora\", constructed_corpus_dir=\"constructed_language_corpora\"):\n",
    "        # Setup directories\n",
    "        self.natural_corpus_dir = Path(natural_corpus_dir)\n",
    "        self.constructed_corpus_dir = Path(constructed_corpus_dir)\n",
    "        self.natural_corpus_dir.mkdir(exist_ok=True)\n",
    "        self.constructed_corpus_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Collection target\n",
    "        self.target_words = 20000\n",
    "        self.min_words = 5000\n",
    "        \n",
    "        # Define languages\n",
    "        self.languages = {\n",
    "            # Natural languages\n",
    "            'english': {'type': 'natural', 'iso': 'en', 'wiki': 'en', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'spanish': {'type': 'natural', 'iso': 'es', 'wiki': 'es', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'french': {'type': 'natural', 'iso': 'fr', 'wiki': 'fr', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'german': {'type': 'natural', 'iso': 'de', 'wiki': 'de', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'russian': {'type': 'natural', 'iso': 'ru', 'wiki': 'ru', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'turkish': {'type': 'natural', 'iso': 'tr', 'wiki': 'tr', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'finnish': {'type': 'natural', 'iso': 'fi', 'wiki': 'fi', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'hungarian': {'type': 'natural', 'iso': 'hu', 'wiki': 'hu', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'basque': {'type': 'natural', 'iso': 'eu', 'wiki': 'eu', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'polish': {'type': 'natural', 'iso': 'pl', 'wiki': 'pl', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'bulgarian': {'type': 'natural', 'iso': 'bg', 'wiki': 'bg', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'greek': {'type': 'natural', 'iso': 'el', 'wiki': 'el', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'albanian': {'type': 'natural', 'iso': 'sq', 'wiki': 'sq', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'romanian': {'type': 'natural', 'iso': 'ro', 'wiki': 'ro', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'irish': {'type': 'natural', 'iso': 'ga', 'wiki': 'ga', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'maltese': {'type': 'natural', 'iso': 'mt', 'wiki': 'mt', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'arabic': {'type': 'natural', 'iso': 'ar', 'wiki': 'ar', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'hebrew': {'type': 'natural', 'iso': 'he', 'wiki': 'he', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'latin': {'type': 'natural', 'iso': 'la', 'wiki': 'la', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            'italian': {'type': 'natural', 'iso': 'la', 'wiki': 'it', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature']},\n",
    "            # Constructed languages\n",
    "            'simple_english': {'type': 'constructed', 'iso': 'en', 'wiki': 'simple', 'search_terms': ['history', 'culture', 'geography', 'science', 'literature', 'art']},\n",
    "            'esperanto': {'type': 'constructed', 'iso': 'eo', 'wiki': 'eo', 'search_terms': ['historio', 'kulturo', 'geografio', 'scienco', 'literaturo', 'arto']},\n",
    "            'lingua_franca_nova': {'type': 'constructed', 'iso': 'lfn', 'wiki': 'lfn', 'search_terms': ['istoria', 'cultur', 'jeografia', 'siensa', 'leteratur', 'arte']}\n",
    "        }\n",
    "\n",
    "    def get_status(self):\n",
    "        \"\"\"Check the status of all language corpora.\"\"\"\n",
    "        status = {}\n",
    "        for lang, config in self.languages.items():\n",
    "            dir_path = self.natural_corpus_dir if config['type'] == 'natural' else self.constructed_corpus_dir\n",
    "            file_path = dir_path / f\"{lang}_corpus.txt\"\n",
    "            if file_path.exists():\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                word_count = len(text.split())\n",
    "                status[lang] = {'exists': True, 'words': word_count, 'needs_collection': word_count < self.min_words}\n",
    "            else:\n",
    "                status[lang] = {'exists': False, 'words': 0, 'needs_collection': True}\n",
    "        return status\n",
    "    \n",
    "    def collect_wikipedia_text(self, lang_name):\n",
    "        \"\"\"Collect Wikipedia text for a language.\"\"\"\n",
    "        lang_config = self.languages[lang_name]\n",
    "        wikipedia.set_lang(lang_config['wiki'])\n",
    "        \n",
    "        collected_texts = []\n",
    "        total_words = 0\n",
    "        search_terms = lang_config['search_terms']\n",
    "        results_limit = 2 if lang_config['type'] == 'natural' else 5\n",
    "        min_content_length = 1000 if lang_config['type'] == 'natural' else 150\n",
    "        \n",
    "        for term in search_terms:\n",
    "            if total_words >= self.target_words:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                search_results = wikipedia.search(term, results=results_limit)\n",
    "                for title in search_results:\n",
    "                    if total_words >= self.target_words:\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        page = wikipedia.page(title, auto_suggest=False)\n",
    "                        content = page.content\n",
    "                        word_count = len(content.split())\n",
    "                        \n",
    "                        if word_count > min_content_length:\n",
    "                            collected_texts.append(content)\n",
    "                            total_words += word_count\n",
    "                            print(f\"    Got '{title}' ({word_count} words). Total: {total_words}\")\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                    except (wikipedia.exceptions.PageError, wikipedia.exceptions.DisambiguationError) as e:\n",
    "                        print(f\"    Skipping '{title}' ({type(e).__name__}).\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Unknown error with '{title}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error searching '{term}': {e}\")\n",
    "        \n",
    "        return '\\n\\n==========\\n\\n'.join(collected_texts), total_words\n",
    "    \n",
    "    def collect_language(self, lang_name):\n",
    "        \"\"\"Collect corpus for one language.\"\"\"\n",
    "        lang_config = self.languages[lang_name]\n",
    "        print(f\"\\n--- Collecting for {lang_name.replace('_', ' ').title()} ({lang_config['wiki']}) ---\")\n",
    "        \n",
    "        # Collect Wikipedia text\n",
    "        text, total_words = self.collect_wikipedia_text(lang_name)\n",
    "        print(f\"  Wikipedia: {total_words} words\")\n",
    "        \n",
    "        # Save if enough words (same for both language types)\n",
    "        dir_path = self.natural_corpus_dir if lang_config['type'] == 'natural' else self.constructed_corpus_dir\n",
    "        if total_words >= self.min_words:\n",
    "            file_path = dir_path / f\"{lang_name}_corpus.txt\"\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            print(f\"Saved: {total_words} words\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed: only {total_words} words\")\n",
    "            return False\n",
    "    \n",
    "    def collect_all(self):\n",
    "        \"\"\"Collect corpora for languages needing collection.\"\"\"\n",
    "        status = self.get_status()\n",
    "        to_collect = [lang for lang, info in status.items() if info['needs_collection']]\n",
    "        \n",
    "        if not to_collect:\n",
    "            print(\"All languages already collected!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Need to collect: {len(to_collect)} languages\")\n",
    "        success_count = 0\n",
    "        for lang in to_collect:\n",
    "            if self.collect_language(lang):\n",
    "                success_count += 1\n",
    "        \n",
    "        print(f\"\\nDone! Successfully collected {success_count}/{len(to_collect)} languages\")\n",
    "    \n",
    "    def add_language(self, name, iso_code, wiki_code=None, search_terms=None, is_constructed=False):\n",
    "        if wiki_code is None:\n",
    "            wiki_code = iso_code\n",
    "        lang_type = 'constructed' if is_constructed else 'natural'\n",
    "        search_terms = search_terms or ['history', 'culture', 'geography', 'science', 'literature']\n",
    "        self.languages[name] = {'type': lang_type, 'iso': iso_code, 'wiki': wiki_code, 'search_terms': search_terms}\n",
    "        print(f\"Added {name} ({iso_code}, {lang_type})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collector = UniversalCorpusCollector()\n",
    "    print(\"Current status:\")\n",
    "    for lang, info in collector.get_status().items():\n",
    "        status_text = f\"{info['words']} words\" if info['exists'] else \"missing\"\n",
    "        print(f\"  {lang}: {status_text}\")\n",
    "    collector.collect_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7ae49",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c4f9a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Zipf's Law\n",
    "\n",
    "Zipf's Law states that in any natural language corpus, the frequency of a word is inversely proportional to its rank—meaning the most frequent word appears twice as often as the second most frequent, three times as often as the third, and so forth. This mathematical principle is consistent across languages and provides a powerful tool for language decipherment because it allows researchers to identify the most common functional words even without understanding their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdaa56-2dd4-4715-9040-c2206fdf9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import re         \n",
    "import numpy as np        \n",
    "import pandas as pd     \n",
    "import matplotlib.pyplot as plt  \n",
    "from collections import Counter  \n",
    "from scipy import stats     \n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db9e76-8c31-4656-b5ac-617a2dc70bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreZipfAnalyzer:\n",
    "    # Initialize the analyzer with empty attributes\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.frequency_data = None\n",
    "        self.zipf_results = {}\n",
    "    \n",
    "    # Define the Zipf's law function: f(r) = C / r^alpha\n",
    "    def zipf_function(self, rank, C, alpha):\n",
    "        # rank: word rank (1 = most frequent, 2 = second, etc.)\n",
    "        # C: scaling constant\n",
    "        # alpha: exponent (typically ~1 for natural languages)\n",
    "        return C / (rank ** alpha)\n",
    "    \n",
    "    # Clean text by removing numbers, punctuation, and normalizing whitespace\n",
    "    def clean_generic_text(self, text):\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text.lower()\n",
    "    \n",
    "    # Tokenize text into words, filtering out short tokens\n",
    "    def tokenize_generic_text(self, text):\n",
    "        text = self.clean_generic_text(text)\n",
    "        tokens = text.split() \n",
    "        tokens = [token for token in tokens if len(token) >= 2]\n",
    "        return tokens\n",
    "    \n",
    "    # Calculate word frequencies and ranks\n",
    "    def calculate_frequencies(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        freq_counter = Counter(tokens)\n",
    "        \n",
    "        freq_data = pd.DataFrame([\n",
    "            {'token': token, 'frequency': freq}\n",
    "            for token, freq in freq_counter.items()\n",
    "        ])\n",
    "        \n",
    "        # Sort by frequency (descending), add rank, and compute metrics\n",
    "        freq_data = freq_data.sort_values('frequency', ascending=False).reset_index(drop=True)\n",
    "        freq_data['rank'] = range(1, len(freq_data) + 1)  # 1-based rank\n",
    "        freq_data['relative_frequency'] = freq_data['frequency'] / len(tokens)\n",
    "        freq_data['freq_rank_product'] = freq_data['frequency'] * freq_data['rank']  # Zipf product\n",
    "        \n",
    "        self.frequency_data = freq_data\n",
    "        return freq_data\n",
    "    \n",
    "    # Fit Zipf's law to frequency data\n",
    "    def fit_zipf_law(self):\n",
    "        if self.frequency_data is None:\n",
    "            return None\n",
    "        \n",
    "        ranks = self.frequency_data['rank'].values\n",
    "        frequencies = self.frequency_data['frequency'].values\n",
    "        \n",
    "        try:\n",
    "            # Initial guesses for curve fitting\n",
    "            C_guess = frequencies[0]  # Top word's frequency\n",
    "            alpha_guess = 1.0  # Zipf exponent\n",
    "            \n",
    "            # Fit Zipf function to data\n",
    "            popt, pcov = curve_fit(self.zipf_function, ranks, frequencies, \n",
    "                                 p0=[C_guess, alpha_guess], maxfev=5000)\n",
    "            \n",
    "            C_fitted, alpha_fitted = popt\n",
    "            predicted_freq = self.zipf_function(ranks, C_fitted, alpha_fitted)\n",
    "            \n",
    "            # Calculate goodness of fit\n",
    "            r_squared = stats.pearsonr(frequencies, predicted_freq)[0] ** 2  # R²\n",
    "            rmse = np.sqrt(np.mean((frequencies - predicted_freq) ** 2))  # Root Mean Squared Error\n",
    "            \n",
    "            # Log-log linear regression for validation\n",
    "            log_ranks = np.log(ranks)\n",
    "            log_frequencies = np.log(frequencies)\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(log_ranks, log_frequencies)\n",
    "            \n",
    "            # Store results\n",
    "            self.zipf_results = {\n",
    "                'C': C_fitted,\n",
    "                'alpha': alpha_fitted,\n",
    "                'r_squared': r_squared,\n",
    "                'rmse': rmse,\n",
    "                'log_slope': slope,\n",
    "                'log_intercept': intercept,\n",
    "                'log_r_squared': r_value ** 2,\n",
    "                'predicted_frequencies': predicted_freq\n",
    "            }\n",
    "            \n",
    "            return self.zipf_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    # Calculate basic linguistic metrics\n",
    "    def calculate_language_metrics(self):\n",
    "        if self.frequency_data is None:\n",
    "            return None\n",
    "        \n",
    "        total_tokens = len(self.tokens) \n",
    "        unique_tokens = len(self.frequency_data) \n",
    "        type_token_ratio = unique_tokens / total_tokens  # Lexical diversity\n",
    "        frequencies = self.frequency_data['frequency'].values\n",
    "        hapax_count = sum(1 for freq in frequencies if freq == 1)  # Words appearing once\n",
    "        hapax_ratio = hapax_count / unique_tokens  # Proportion of hapax legomena\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': total_tokens,\n",
    "            'unique_tokens': unique_tokens,\n",
    "            'type_token_ratio': type_token_ratio,\n",
    "            'hapax_ratio': hapax_ratio\n",
    "        }\n",
    "    \n",
    "    # Perform full Zipf analysis on input text\n",
    "    def analyze_text(self, text, text_name=\"Text\"):\n",
    "        tokens = self.tokenize_generic_text(text)\n",
    "        if len(tokens) < 100: \n",
    "            return None\n",
    "        \n",
    "        freq_data = self.calculate_frequencies(tokens)\n",
    "        zipf_results = self.fit_zipf_law()\n",
    "        if not zipf_results:\n",
    "            return None\n",
    "        \n",
    "        metrics = self.calculate_language_metrics()\n",
    "        \n",
    "        # Compile all results\n",
    "        complete_results = {\n",
    "            'text_name': text_name,\n",
    "            'tokens': tokens,\n",
    "            'frequency_data': freq_data,\n",
    "            'zipf_results': zipf_results,\n",
    "            'metrics': metrics,\n",
    "            'top_10_words': freq_data.head(10)['token'].tolist()\n",
    "        }\n",
    "        \n",
    "        return complete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf7975-6960-4580-a306-19ba60ca5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialized class for analyzing the Voynich Manuscript with Zipf-Mandelbrot law\n",
    "class VoynichZipfAnalyzer:\n",
    "    def __init__(self, treat_commas_as_spaces=True):\n",
    "        # Initialize with core analyzer and Voynich-specific attributes\n",
    "        self.core_analyzer = CoreZipfAnalyzer()  # Reuse core functionality\n",
    "        self.treat_commas_as_spaces = treat_commas_as_spaces  # Flag for comma handling\n",
    "        self.tokens = []  # Store tokens\n",
    "        self.frequency_data = None  # Store frequency data\n",
    "        self.zipf_results = {}  # Store Zipf-Mandelbrot results\n",
    "        self.cleaned_text = None  # Store preprocessed text\n",
    "    \n",
    "    # Load and preprocess Voynich text from file\n",
    "    def load_and_preprocess_text(self, filepath, treat_commas_as_spaces=True):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()  # Read all lines\n",
    "            \n",
    "            eva_text_segments = []  # Store cleaned text segments\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                \n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Extract EVA text after locus tags (e.g., <f1r.P1.1;H>)\n",
    "                match = re.match(r'<f[^>]+>\\s*(.*)', line)\n",
    "                if match:\n",
    "                    eva_part = match.group(1).strip()\n",
    "                    if eva_part:\n",
    "                        # Process each segment\n",
    "                        processed_eva = self.process_eva_text(eva_part, treat_commas_as_spaces)\n",
    "                        if processed_eva:\n",
    "                            eva_text_segments.append(processed_eva)\n",
    "            \n",
    "            # Join segments into full text\n",
    "            final_text = ' '.join(eva_text_segments)\n",
    "            self.cleaned_text = final_text\n",
    "            return final_text\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    # Clean individual EVA text segments\n",
    "    def process_eva_text(self, eva_text, treat_commas_as_spaces=True):\n",
    "        # Remove reference numbers (e.g., @254)\n",
    "        eva_text = re.sub(r'@\\d+', '', eva_text)\n",
    "        \n",
    "        # Remove complex comments (e.g., <!@254;>)\n",
    "        eva_text = re.sub(r'<![^>]*>', '', eva_text)\n",
    "        \n",
    "        # Remove other tags\n",
    "        eva_text = re.sub(r'<[^>]*>', '', eva_text)\n",
    "        \n",
    "        # Handle uncertain readings [a:b:c] by taking first option\n",
    "        def replace_uncertain(match):\n",
    "            options = match.group(1).split(':')\n",
    "            return options[0] if options else ''\n",
    "        eva_text = re.sub(r'\\[([^\\]]+)\\]', replace_uncertain, eva_text)\n",
    "        \n",
    "        # Remove ligature braces and unreadable glyphs\n",
    "        eva_text = re.sub(r'[{}]', '', eva_text)\n",
    "        eva_text = re.sub(r'\\?+', '', eva_text)\n",
    "        \n",
    "        # Handle word boundaries (dots = word separators)\n",
    "        if treat_commas_as_spaces:\n",
    "            eva_text = eva_text.replace('.', ' ').replace(',', ' ')\n",
    "        else:\n",
    "            eva_text = eva_text.replace('.', ' ').replace(',', '')\n",
    "        \n",
    "        # Normalize whitespace and lowercase\n",
    "        eva_text = re.sub(r'\\s+', ' ', eva_text).strip()\n",
    "        eva_text = eva_text.lower()\n",
    "        \n",
    "        # Keep only letter-only words\n",
    "        words = eva_text.split()\n",
    "        letter_only_words = [word for word in words if re.match(r'^[a-z]+$', word)]\n",
    "        \n",
    "        return ' '.join(letter_only_words)\n",
    "    \n",
    "    # Apply Zipf-Mandelbrot analysis to text\n",
    "    def apply_zipf_mandelbrot_analysis(self, text=None):\n",
    "        if text is None:\n",
    "            if self.cleaned_text is None:\n",
    "                raise ValueError(\"No text provided and no text previously loaded\")\n",
    "            text = self.cleaned_text\n",
    "        \n",
    "        # Tokenize and compute frequencies\n",
    "        self.tokens = self.tokenize_text(text)\n",
    "        self.frequency_data = self.calculate_frequencies()\n",
    "        self.zipf_results = self.fit_zipf_mandelbrot_law()\n",
    "        \n",
    "        return self.zipf_results\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    def tokenize_text(self, text):\n",
    "        if text is None:\n",
    "            return []\n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens if len(token) >= 1]\n",
    "        return tokens\n",
    "    \n",
    "    # Calculate frequencies\n",
    "    def calculate_frequencies(self):\n",
    "        self.frequency_data = self.core_analyzer.calculate_frequencies(self.tokens)\n",
    "        return self.frequency_data\n",
    "    \n",
    "    # Fit Zipf-Mandelbrot law: f(r) = C / (r + β)^α\n",
    "    def fit_zipf_mandelbrot_law(self):\n",
    "        if self.frequency_data is None:\n",
    "            return {}\n",
    "        \n",
    "        ranks = self.frequency_data['rank'].values\n",
    "        frequencies = self.frequency_data['frequency'].values\n",
    "        \n",
    "        # Define Zipf-Mandelbrot function\n",
    "        def zipf_mandelbrot(r, C, alpha, beta):\n",
    "            return C / (r + beta) ** alpha\n",
    "        \n",
    "        try:\n",
    "            # Initial parameter guesses\n",
    "            initial_guess = [frequencies[0], 1.0, 10.0]\n",
    "            \n",
    "            # Fit curve\n",
    "            popt, pcov = curve_fit(zipf_mandelbrot, ranks, frequencies, \n",
    "                                 p0=initial_guess, maxfev=5000)\n",
    "            \n",
    "            C_opt, alpha_opt, beta_opt = popt  # Extract parameters\n",
    "            predicted_freq = zipf_mandelbrot(ranks, C_opt, alpha_opt, beta_opt)  # Predicted frequencies\n",
    "            \n",
    "            # Calculate goodness of fit\n",
    "            ss_res = np.sum((frequencies - predicted_freq) ** 2)\n",
    "            ss_tot = np.sum((frequencies - np.mean(frequencies)) ** 2)\n",
    "            r_squared = 1 - (ss_res / ss_tot)\n",
    "            rmse = np.sqrt(np.mean((frequencies - predicted_freq) ** 2))\n",
    "            \n",
    "            # Log-log regression\n",
    "            log_ranks = np.log(ranks)\n",
    "            log_frequencies = np.log(frequencies)\n",
    "            valid_idx = np.isfinite(log_ranks) & np.isfinite(log_frequencies)\n",
    "            log_ranks_clean = log_ranks[valid_idx]\n",
    "            log_frequencies_clean = log_frequencies[valid_idx]\n",
    "            \n",
    "            if len(log_ranks_clean) > 1:\n",
    "                log_coeffs = np.polyfit(log_ranks_clean, log_frequencies_clean, 1)\n",
    "                log_slope = log_coeffs[0]\n",
    "                log_predicted = np.polyval(log_coeffs, log_ranks_clean)\n",
    "                log_r_squared = 1 - np.sum((log_frequencies_clean - log_predicted) ** 2) / \\\n",
    "                               np.sum((log_frequencies_clean - np.mean(log_frequencies_clean)) ** 2)\n",
    "            else:\n",
    "                log_slope = np.nan\n",
    "                log_r_squared = np.nan\n",
    "            \n",
    "            # Calculate frequency-rank products\n",
    "            freq_rank_products = frequencies * ranks\n",
    "            \n",
    "            # Store results\n",
    "            results = {\n",
    "                'C': C_opt,\n",
    "                'alpha': alpha_opt,\n",
    "                'beta': beta_opt,\n",
    "                'predicted_frequencies': predicted_freq,\n",
    "                'r_squared': r_squared,\n",
    "                'rmse': rmse,\n",
    "                'log_slope': log_slope,\n",
    "                'log_r_squared': log_r_squared,\n",
    "                'parameter_errors': np.sqrt(np.diag(pcov)),\n",
    "                'freq_rank_products': freq_rank_products\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting Zipf-Mandelbrot law: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    # Alias for backward compatibility\n",
    "    def fit_zipf_law(self):\n",
    "        return self.fit_zipf_mandelbrot_law()\n",
    "    \n",
    "    # Plot analysis results\n",
    "    def plot_zipf_analysis(self):\n",
    "        if self.frequency_data is None or not self.zipf_results:\n",
    "            return\n",
    "        \n",
    "        # Create subplot figure\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Zipf-Mandelbrot Law Analysis of Voynich Manuscript', fontsize=16)\n",
    "        \n",
    "        ranks = self.frequency_data['rank'].values\n",
    "        frequencies = self.frequency_data['frequency'].values\n",
    "        predicted_freq = self.zipf_results['predicted_frequencies']\n",
    "        \n",
    "        # Linear scale plot (top 100 ranks)\n",
    "        axes[0, 0].scatter(ranks[:100], frequencies[:100], alpha=0.6, s=30)\n",
    "        axes[0, 0].plot(ranks[:100], predicted_freq[:100], 'r-', linewidth=2, label='Fitted Zipf-Mandelbrot curve')\n",
    "        axes[0, 0].set_xlabel('Rank')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Frequency vs Rank (Linear Scale)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Log-log plot\n",
    "        axes[0, 1].loglog(ranks, frequencies, 'bo', alpha=0.6, markersize=4, label='Observed')\n",
    "        axes[0, 1].loglog(ranks, predicted_freq, 'r-', linewidth=2, label='Fitted Zipf-Mandelbrot curve')\n",
    "        axes[0, 1].set_xlabel('Rank (log scale)')\n",
    "        axes[0, 1].set_ylabel('Frequency (log scale)')\n",
    "        axes[0, 1].set_title('Log-Log Plot')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals plot\n",
    "        residuals = frequencies - predicted_freq\n",
    "        axes[1, 0].scatter(ranks, residuals, alpha=0.6, s=20)\n",
    "        axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "        axes[1, 0].set_xlabel('Rank')\n",
    "        axes[1, 0].set_ylabel('Residuals (Observed - Predicted)')\n",
    "        axes[1, 0].set_title('Residuals Plot')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Frequency-rank product plot\n",
    "        freq_rank_products = self.zipf_results['freq_rank_products']\n",
    "        axes[1, 1].plot(ranks[:100], freq_rank_products[:100], 'g-', alpha=0.7, linewidth=2)\n",
    "        axes[1, 1].axhline(y=np.mean(freq_rank_products), color='r', linestyle='--', \n",
    "                          label=f'Mean = {np.mean(freq_rank_products):.1f}')\n",
    "        axes[1, 1].set_xlabel('Rank')\n",
    "        axes[1, 1].set_ylabel('Frequency × Rank')\n",
    "        axes[1, 1].set_title('Zipf Product (should be constant)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "voynich_analyzer = VoynichZipfAnalyzer(treat_commas_as_spaces=True)\n",
    "\n",
    "# Load and preprocess Voynich text\n",
    "text = voynich_analyzer.load_and_preprocess_text(\"transliteration_zl.txt\")\n",
    "\n",
    "if text is not None:\n",
    "    zipf_results = voynich_analyzer.apply_zipf_mandelbrot_analysis()\n",
    "    metrics = voynich_analyzer.core_analyzer.calculate_language_metrics()\n",
    "    \n",
    "    voynich_df = pd.DataFrame({\n",
    "        'Language': ['Voynich Manuscript'],\n",
    "        'Type': ['Unknown'],\n",
    "        'Total_Tokens': [metrics['total_tokens']],\n",
    "        'Unique_Tokens': [metrics['unique_tokens']],\n",
    "        'Type_Token_Ratio': [metrics['type_token_ratio']],\n",
    "        'Hapax_Ratio': [metrics['hapax_ratio']],\n",
    "        'Zipf_C': [zipf_results['C']],\n",
    "        'Zipf_Alpha': [zipf_results['alpha']],\n",
    "        'Zipf_Beta': [zipf_results['beta']],\n",
    "        'R_Squared': [zipf_results['r_squared']],\n",
    "        'RMSE': [zipf_results['rmse']],\n",
    "        'Log_Slope': [zipf_results['log_slope']],\n",
    "        'Log_R_Squared': [zipf_results['log_r_squared']]\n",
    "    })\n",
    "    \n",
    "    # Print key results\n",
    "    print(\"Voynich Zipf-Mandelbrot Analysis Results:\")\n",
    "    print(f\"C (constant): {zipf_results['C']:.2f}\")\n",
    "    print(f\"α (exponent): {zipf_results['alpha']:.2f}\")\n",
    "    print(f\"β (offset): {zipf_results['beta']:.2f}\")\n",
    "    print(f\"R²: {zipf_results['r_squared']:.4f}\")\n",
    "    print(f\"RMSE: {zipf_results['rmse']:.4f}\")\n",
    "    display(voynich_df)\n",
    "    \n",
    "    voynich_analyzer.plot_zipf_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a84594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:51:23.813735Z",
     "start_time": "2025-09-03T05:51:21.350301Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "class VoynichZipfAnalyzer:\n",
    "    def __init__(self, treat_commas_as_spaces=True):\n",
    "        self.core_analyzer = CoreZipfAnalyzer()\n",
    "        self.treat_commas_as_spaces = treat_commas_as_spaces\n",
    "        self.tokens = []\n",
    "        self.frequency_data = None\n",
    "        self.zipf_results = {}\n",
    "        self.cleaned_text = None\n",
    "    \n",
    "    def load_and_preprocess_text(self, filepath, treat_commas_as_spaces=True):\n",
    "        \"\"\"\n",
    "        Load and clean the Voynich manuscript text.\n",
    "        Returns cleaned text for use in other functions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            eva_text_segments = []\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Skip empty lines and comment lines\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Look for lines that contain locus identifiers and extract EVA text\n",
    "                match = re.match(r'<f[^>]+>\\s*(.*)', line)\n",
    "                if match:\n",
    "                    eva_part = match.group(1).strip()\n",
    "                    if eva_part:\n",
    "                        processed_eva = self.process_eva_text(eva_part, treat_commas_as_spaces)\n",
    "                        \n",
    "                        if processed_eva:\n",
    "                            eva_text_segments.append(processed_eva)\n",
    "            \n",
    "            final_text = ' '.join(eva_text_segments)\n",
    "            self.cleaned_text = final_text\n",
    "            return final_text\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def process_eva_text(self, eva_text, treat_commas_as_spaces=True):\n",
    "        \"\"\"Helper method to process individual EVA text segments.\"\"\"\n",
    "        # Remove inline reference numbers like @254, @192\n",
    "        eva_text = re.sub(r'@\\d+', '', eva_text)\n",
    "        \n",
    "        # Remove complex comments like <!@254;>\n",
    "        eva_text = re.sub(r'<![^>]*>', '', eva_text)\n",
    "        \n",
    "        # Remove other inline comments\n",
    "        eva_text = re.sub(r'<[^>]*>', '', eva_text)\n",
    "        \n",
    "        # Handle uncertain readings [a:b:c] - keep first option\n",
    "        def replace_uncertain(match):\n",
    "            options = match.group(1).split(':')\n",
    "            return options[0] if options else ''\n",
    "        eva_text = re.sub(r'\\[([^\\]]+)\\]', replace_uncertain, eva_text)\n",
    "        \n",
    "        # Remove ligature braces but keep content\n",
    "        eva_text = re.sub(r'[{}]', '', eva_text)\n",
    "        \n",
    "        # Remove question marks (unreadable glyphs)\n",
    "        eva_text = re.sub(r'\\?+', '', eva_text)\n",
    "        \n",
    "        # Handle word boundaries - dots separate words in EVA\n",
    "        if treat_commas_as_spaces:\n",
    "            eva_text = eva_text.replace('.', ' ').replace(',', ' ')\n",
    "        else:\n",
    "            eva_text = eva_text.replace('.', ' ').replace(',', '')\n",
    "        \n",
    "        # Clean up whitespace and normalize case\n",
    "        eva_text = re.sub(r'\\s+', ' ', eva_text).strip()\n",
    "        eva_text = eva_text.lower()\n",
    "        \n",
    "        # Filter out words that are not constituted with only letters\n",
    "        words = eva_text.split()\n",
    "        letter_only_words = []\n",
    "        for word in words:\n",
    "            if re.match(r'^[a-z]+$', word):\n",
    "                letter_only_words.append(word)\n",
    "        \n",
    "        return ' '.join(letter_only_words)\n",
    "    \n",
    "    def apply_zipf_mandelbrot_analysis(self, text=None):\n",
    "        \"\"\"\n",
    "        Apply Zipf-Mandelbrot law analysis to the text.\n",
    "        If no text provided, uses previously loaded text.\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            if self.cleaned_text is None:\n",
    "                raise ValueError(\"No text provided and no text previously loaded\")\n",
    "            text = self.cleaned_text\n",
    "        \n",
    "        # Tokenize and calculate frequencies\n",
    "        self.tokens = self.tokenize_text(text)\n",
    "        self.frequency_data = self.calculate_frequencies()\n",
    "        self.zipf_results = self.fit_zipf_mandelbrot_law()\n",
    "        \n",
    "        return self.zipf_results\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        if text is None:\n",
    "            return []\n",
    "        \n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens if len(token) >= 1]\n",
    "        return tokens\n",
    "    \n",
    "    def calculate_frequencies(self):\n",
    "        self.frequency_data = self.core_analyzer.calculate_frequencies(self.tokens)\n",
    "        return self.frequency_data\n",
    "    \n",
    "    def fit_zipf_mandelbrot_law(self):\n",
    "        \"\"\"\n",
    "        Fit Zipf-Mandelbrot law: f(r) = C / (r + β)^α\n",
    "        where C is a constant, β is the offset parameter, and α is the exponent\n",
    "        \"\"\"\n",
    "        if self.frequency_data is None:\n",
    "            return {}\n",
    "        \n",
    "        ranks = self.frequency_data['rank'].values\n",
    "        frequencies = self.frequency_data['frequency'].values\n",
    "        \n",
    "        # Zipf-Mandelbrot function\n",
    "        def zipf_mandelbrot(r, C, alpha, beta):\n",
    "            return C / (r + beta) ** alpha\n",
    "        \n",
    "        try:\n",
    "            # Initial parameter guesses\n",
    "            initial_guess = [frequencies[0], 1.0, 10.0]\n",
    "            \n",
    "            # Fit the curve\n",
    "            popt, pcov = curve_fit(zipf_mandelbrot, ranks, frequencies, \n",
    "                                 p0=initial_guess, maxfev=5000)\n",
    "            \n",
    "            C_opt, alpha_opt, beta_opt = popt\n",
    "            \n",
    "            # Calculate predicted frequencies\n",
    "            predicted_freq = zipf_mandelbrot(ranks, C_opt, alpha_opt, beta_opt)\n",
    "            \n",
    "            # Calculate goodness of fit metrics\n",
    "            ss_res = np.sum((frequencies - predicted_freq) ** 2)\n",
    "            ss_tot = np.sum((frequencies - np.mean(frequencies)) ** 2)\n",
    "            r_squared = 1 - (ss_res / ss_tot)\n",
    "            \n",
    "            rmse = np.sqrt(np.mean((frequencies - predicted_freq) ** 2))\n",
    "            \n",
    "            # Log-log regression for comparison\n",
    "            log_ranks = np.log(ranks)\n",
    "            log_frequencies = np.log(frequencies)\n",
    "            \n",
    "            # Remove any infinite or NaN values\n",
    "            valid_idx = np.isfinite(log_ranks) & np.isfinite(log_frequencies)\n",
    "            log_ranks_clean = log_ranks[valid_idx]\n",
    "            log_frequencies_clean = log_frequencies[valid_idx]\n",
    "            \n",
    "            if len(log_ranks_clean) > 1:\n",
    "                log_coeffs = np.polyfit(log_ranks_clean, log_frequencies_clean, 1)\n",
    "                log_slope = log_coeffs[0]\n",
    "                log_predicted = np.polyval(log_coeffs, log_ranks_clean)\n",
    "                log_r_squared = 1 - np.sum((log_frequencies_clean - log_predicted) ** 2) / \\\n",
    "                               np.sum((log_frequencies_clean - np.mean(log_frequencies_clean)) ** 2)\n",
    "            else:\n",
    "                log_slope = np.nan\n",
    "                log_r_squared = np.nan\n",
    "            \n",
    "            # Calculate frequency-rank products for Zipf analysis\n",
    "            freq_rank_products = frequencies * ranks\n",
    "            \n",
    "            results = {\n",
    "                'C': C_opt,\n",
    "                'alpha': alpha_opt,\n",
    "                'beta': beta_opt,\n",
    "                'predicted_frequencies': predicted_freq,\n",
    "                'r_squared': r_squared,\n",
    "                'rmse': rmse,\n",
    "                'log_slope': log_slope,\n",
    "                'log_r_squared': log_r_squared,\n",
    "                'parameter_errors': np.sqrt(np.diag(pcov)),\n",
    "                'freq_rank_products': freq_rank_products\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting Zipf-Mandelbrot law: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def fit_zipf_law(self):\n",
    "        \"\"\"\n",
    "        Maintained for backward compatibility - now calls Zipf-Mandelbrot\n",
    "        \"\"\"\n",
    "        return self.fit_zipf_mandelbrot_law()\n",
    "    \n",
    "    def plot_zipf_analysis(self):\n",
    "        if self.frequency_data is None or not self.zipf_results:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Zipf-Mandelbrot Law Analysis of Voynich Manuscript', fontsize=16)\n",
    "        \n",
    "        ranks = self.frequency_data['rank'].values\n",
    "        frequencies = self.frequency_data['frequency'].values\n",
    "        predicted_freq = self.zipf_results['predicted_frequencies']\n",
    "        \n",
    "        # Plot 1: Linear scale\n",
    "        axes[0, 0].scatter(ranks[:100], frequencies[:100], alpha=0.6, s=30)\n",
    "        axes[0, 0].plot(ranks[:100], predicted_freq[:100], 'r-', linewidth=2, label='Fitted Zipf-Mandelbrot curve')\n",
    "        axes[0, 0].set_xlabel('Rank')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Frequency vs Rank (Linear Scale)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Log-log plot\n",
    "        axes[0, 1].loglog(ranks, frequencies, 'bo', alpha=0.6, markersize=4, label='Observed')\n",
    "        axes[0, 1].loglog(ranks, predicted_freq, 'r-', linewidth=2, label='Fitted Zipf-Mandelbrot curve')\n",
    "        axes[0, 1].set_xlabel('Rank (log scale)')\n",
    "        axes[0, 1].set_ylabel('Frequency (log scale)')\n",
    "        axes[0, 1].set_title('Log-Log Plot')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Residuals\n",
    "        residuals = frequencies - predicted_freq\n",
    "        axes[1, 0].scatter(ranks, residuals, alpha=0.6, s=20)\n",
    "        axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "        axes[1, 0].set_xlabel('Rank')\n",
    "        axes[1, 0].set_ylabel('Residuals (Observed - Predicted)')\n",
    "        axes[1, 0].set_title('Residuals Plot')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Frequency rank product\n",
    "        freq_rank_products = self.zipf_results['freq_rank_products']\n",
    "        axes[1, 1].plot(ranks[:100], freq_rank_products[:100], 'g-', alpha=0.7, linewidth=2)\n",
    "        axes[1, 1].axhline(y=np.mean(freq_rank_products), color='r', linestyle='--', \n",
    "                          label=f'Mean = {np.mean(freq_rank_products):.1f}')\n",
    "        axes[1, 1].set_xlabel('Rank')\n",
    "        axes[1, 1].set_ylabel('Frequency × Rank')\n",
    "        axes[1, 1].set_title('Zipf Product (should be constant)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "voynich_analyzer = VoynichZipfAnalyzer(treat_commas_as_spaces=True)\n",
    "\n",
    "# Load and clean the text\n",
    "text = voynich_analyzer.load_and_preprocess_text(\"transliteration_zl.txt\")\n",
    "\n",
    "if text is not None:\n",
    "    # Apply Zipf-Mandelbrot analysis\n",
    "    zipf_results = voynich_analyzer.apply_zipf_mandelbrot_analysis()\n",
    "    metrics = voynich_analyzer.core_analyzer.calculate_language_metrics()\n",
    "    \n",
    "    # Create Voynich results DataFrame\n",
    "    voynich_df = pd.DataFrame({\n",
    "        'Language': ['Voynich Manuscript'],\n",
    "        'Type': ['Unknown'],\n",
    "        'Total_Tokens': [metrics['total_tokens']],\n",
    "        'Unique_Tokens': [metrics['unique_tokens']],\n",
    "        'Type_Token_Ratio': [metrics['type_token_ratio']],\n",
    "        'Hapax_Ratio': [metrics['hapax_ratio']],\n",
    "        'Zipf_C': [zipf_results['C']],\n",
    "        'Zipf_Alpha': [zipf_results['alpha']],\n",
    "        'Zipf_Beta': [zipf_results['beta']],\n",
    "        'R_Squared': [zipf_results['r_squared']],\n",
    "        'RMSE': [zipf_results['rmse']],\n",
    "        'Log_Slope': [zipf_results['log_slope']],\n",
    "        'Log_R_Squared': [zipf_results['log_r_squared']]\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Voynich Zipf-Mandelbrot Analysis Results:\")\n",
    "    print(f\"C (constant): {zipf_results['C']:.2f}\")\n",
    "    print(f\"α (exponent): {zipf_results['alpha']:.2f}\")\n",
    "    print(f\"β (offset): {zipf_results['beta']:.2f}\")\n",
    "    print(f\"R²: {zipf_results['r_squared']:.4f}\")\n",
    "    print(f\"RMSE: {zipf_results['rmse']:.4f}\")\n",
    "    display(voynich_df)\n",
    "    \n",
    "    # Show plots\n",
    "    voynich_analyzer.plot_zipf_analysis()\n",
    "    \n",
    "else:\n",
    "    print(\"Could not load Voynich file 'transliteration_zl.txt'\")\n",
    "    voynich_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf4a83-3d40-4b69-8f1a-e9cc85a924c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_log_log_only(frequency_data, zipf_results):\n",
    "    \"\"\"\n",
    "    Plot only the log-log plot for Zipf-Mandelbrot analysis.\n",
    "    Requires frequency_data and zipf_results from VoynichZipfAnalyzer.\n",
    "    \"\"\"\n",
    "    if frequency_data is None or not zipf_results:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    ranks = frequency_data['rank'].values\n",
    "    frequencies = frequency_data['frequency'].values\n",
    "    predicted_freq = zipf_results['predicted_frequencies']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.loglog(ranks, frequencies, 'bo', alpha=0.6, markersize=4, label='Observed')\n",
    "    plt.loglog(ranks, predicted_freq, 'r-', linewidth=2, label='Fitted Zipf-Mandelbrot curve')\n",
    "    plt.xlabel('Rank (log scale)')\n",
    "    plt.ylabel('Frequency (log scale)')\n",
    "    plt.title('Log-Log Plot of Voynich Manuscript Word Frequencies')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming voynich_analyzer is already initialized and analyzed)\n",
    "plot_log_log_only(voynich_analyzer.frequency_data, voynich_analyzer.zipf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80d755-ec5a-485f-9e4b-ca3b89652a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageCorpusAnalyzer:\n",
    "    def __init__(self, natural_corpus_dir=\"language_corpora\", constructed_corpus_dir=\"constructed_language_corpora\"):\n",
    "        self.corpus_dirs = [Path(natural_corpus_dir), Path(constructed_corpus_dir)]\n",
    "        self.language_results = {}\n",
    "        self.core_analyzer = CoreZipfAnalyzer()\n",
    "    \n",
    "    def analyze_language_corpus(self, text, language_name):\n",
    "        results = self.core_analyzer.analyze_text(text, language_name)\n",
    "        \n",
    "        if results is None:\n",
    "            return None\n",
    "        \n",
    "        zipf_results = results['zipf_results']\n",
    "        metrics = results['metrics']\n",
    "        \n",
    "        return {\n",
    "            'language': language_name,\n",
    "            'total_tokens': metrics['total_tokens'],\n",
    "            'unique_tokens': metrics['unique_tokens'],\n",
    "            'type_token_ratio': metrics['type_token_ratio'],\n",
    "            'hapax_ratio': metrics['hapax_ratio'],\n",
    "            'zipf_C': zipf_results['C'],\n",
    "            'zipf_alpha': zipf_results['alpha'],\n",
    "            'r_squared': zipf_results['r_squared'],\n",
    "            'rmse': zipf_results['rmse'],\n",
    "            'log_slope': zipf_results['log_slope'],\n",
    "            'log_r_squared': zipf_results['log_r_squared'],\n",
    "            'frequency_data': results['frequency_data'],\n",
    "            'predicted_frequencies': zipf_results['predicted_frequencies'],\n",
    "            'top_10_words': results['top_10_words']\n",
    "        }\n",
    "    \n",
    "    def analyze_all_languages(self):\n",
    "        corpus_files = []\n",
    "        for corpus_dir in self.corpus_dirs:\n",
    "            corpus_files.extend(list(corpus_dir.glob(\"*_corpus.txt\")))\n",
    "        \n",
    "        if not corpus_files:\n",
    "            return None, None\n",
    "        \n",
    "        successful_results = []\n",
    "        \n",
    "        for corpus_file in corpus_files:\n",
    "            language_name = corpus_file.stem.replace('_corpus', '')\n",
    "            \n",
    "            try:\n",
    "                with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                results = self.analyze_language_corpus(text, language_name.title())\n",
    "                \n",
    "                if results:\n",
    "                    successful_results.append(results)\n",
    "                    self.language_results[language_name] = results\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not successful_results:\n",
    "            return None, None\n",
    "        \n",
    "        # Create languages results DataFrame\n",
    "        languages_df = pd.DataFrame([\n",
    "            {\n",
    "                'Language': result['language'],\n",
    "                'Type': 'Natural' if 'language_corpora' in str(corpus_file) else 'Constructed',\n",
    "                'Total_Tokens': result['total_tokens'],\n",
    "                'Unique_Tokens': result['unique_tokens'],\n",
    "                'Type_Token_Ratio': result['type_token_ratio'],\n",
    "                'Hapax_Ratio': result['hapax_ratio'],\n",
    "                'Zipf_C': result['zipf_C'],\n",
    "                'Zipf_Alpha': result['zipf_alpha'],\n",
    "                'R_Squared': result['r_squared'],\n",
    "                'RMSE': result['rmse'],\n",
    "                'Log_Slope': result['log_slope'],\n",
    "                'Log_R_Squared': result['log_r_squared']\n",
    "            }\n",
    "            for result in successful_results\n",
    "        ])\n",
    "        \n",
    "        # Create language statistics DataFrame\n",
    "        stats_df = pd.DataFrame({\n",
    "            'Metric': ['Zipf_Alpha', 'R_Squared', 'Type_Token_Ratio', 'Hapax_Ratio'],\n",
    "            'Mean': [\n",
    "                languages_df['Zipf_Alpha'].mean(),\n",
    "                languages_df['R_Squared'].mean(),\n",
    "                languages_df['Type_Token_Ratio'].mean(),\n",
    "                languages_df['Hapax_Ratio'].mean()\n",
    "            ],\n",
    "            'Std': [\n",
    "                languages_df['Zipf_Alpha'].std(),\n",
    "                languages_df['R_Squared'].std(),\n",
    "                languages_df['Type_Token_Ratio'].std(),\n",
    "                languages_df['Hapax_Ratio'].std()\n",
    "            ],\n",
    "            'Min': [\n",
    "                languages_df['Zipf_Alpha'].min(),\n",
    "                languages_df['R_Squared'].min(),\n",
    "                languages_df['Type_Token_Ratio'].min(),\n",
    "                languages_df['Hapax_Ratio'].min()\n",
    "            ],\n",
    "            'Max': [\n",
    "                languages_df['Zipf_Alpha'].max(),\n",
    "                languages_df['R_Squared'].max(),\n",
    "                languages_df['Type_Token_Ratio'].max(),\n",
    "                languages_df['Hapax_Ratio'].max()\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        return languages_df, stats_df\n",
    "\n",
    "# Analyze all language corpora\n",
    "language_analyzer = LanguageCorpusAnalyzer()\n",
    "languages_df, language_stats_df = language_analyzer.analyze_all_languages()\n",
    "\n",
    "if languages_df is not None:\n",
    "    print(f\"Analyzed {len(languages_df)} languages successfully\")\n",
    "    \n",
    "    print(\"\\nLanguage Analysis Results:\")\n",
    "    display(languages_df)\n",
    "    \n",
    "    print(\"\\nLanguage Statistics:\")\n",
    "    display(language_stats_df)\n",
    "    \n",
    "else:\n",
    "    print(\"No language corpora found or analysis failed\")\n",
    "    languages_df = None\n",
    "    language_stats_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7115ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:51:26.495675Z",
     "start_time": "2025-09-03T05:51:25.330861Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Combine and compare all results\n",
    "if voynich_df is not None and languages_df is not None:\n",
    "    \n",
    "    # Combined results DataFrame (Voynich + all languages)\n",
    "    combined_df = pd.concat([voynich_df, languages_df], ignore_index=True)\n",
    "    \n",
    "    # Similarity DataFrame (all languages ranked by similarity to Voynich)\n",
    "    voynich_alpha = voynich_df['Zipf_Alpha'].iloc[0]\n",
    "    voynich_r2 = voynich_df['R_Squared'].iloc[0]\n",
    "    voynich_ttr = voynich_df['Type_Token_Ratio'].iloc[0]\n",
    "    \n",
    "    similarity_scores = []\n",
    "    for _, lang in languages_df.iterrows():\n",
    "        alpha_diff = abs(lang['Zipf_Alpha'] - voynich_alpha)\n",
    "        r2_diff = abs(lang['R_Squared'] - voynich_r2)\n",
    "        ttr_diff = abs(lang['Type_Token_Ratio'] - voynich_ttr)\n",
    "        \n",
    "        similarity_score = alpha_diff + r2_diff + (ttr_diff * 2)\n",
    "        \n",
    "        similarity_scores.append({\n",
    "            'Language': lang['Language'],\n",
    "            'Similarity_Score': similarity_score,\n",
    "            'Alpha_Difference': alpha_diff,\n",
    "            'R2_Difference': r2_diff,\n",
    "            'TTR_Difference': ttr_diff,\n",
    "            'Zipf_Alpha': lang['Zipf_Alpha'],\n",
    "            'R_Squared': lang['R_Squared'],\n",
    "            'Type_Token_Ratio': lang['Type_Token_Ratio']\n",
    "        })\n",
    "    \n",
    "    similarity_df = pd.DataFrame(similarity_scores).sort_values('Similarity_Score')\n",
    "    \n",
    "    # Fit quality DataFrame (all languages ranked by R²)\n",
    "    fit_quality_df = languages_df[['Language', 'Zipf_Alpha', 'R_Squared', 'RMSE', 'Type_Token_Ratio']].sort_values('R_Squared', ascending=False)\n",
    "    \n",
    "    # Comparison statistics DataFrame\n",
    "    comparison_stats_df = pd.DataFrame({\n",
    "        'Metric': ['Zipf_Alpha', 'R_Squared', 'Type_Token_Ratio', 'Hapax_Ratio'],\n",
    "        'Voynich_Value': [\n",
    "            voynich_df['Zipf_Alpha'].iloc[0],\n",
    "            voynich_df['R_Squared'].iloc[0],\n",
    "            voynich_df['Type_Token_Ratio'].iloc[0],\n",
    "            voynich_df['Hapax_Ratio'].iloc[0]\n",
    "        ],\n",
    "        'Natural_Mean': [\n",
    "            languages_df['Zipf_Alpha'].mean(),\n",
    "            languages_df['R_Squared'].mean(),\n",
    "            languages_df['Type_Token_Ratio'].mean(),\n",
    "            languages_df['Hapax_Ratio'].mean()\n",
    "        ],\n",
    "        'Natural_Min': [\n",
    "            languages_df['Zipf_Alpha'].min(),\n",
    "            languages_df['R_Squared'].min(),\n",
    "            languages_df['Type_Token_Ratio'].min(),\n",
    "            languages_df['Hapax_Ratio'].min()\n",
    "        ],\n",
    "        'Natural_Max': [\n",
    "            languages_df['Zipf_Alpha'].max(),\n",
    "            languages_df['R_Squared'].max(),\n",
    "            languages_df['Type_Token_Ratio'].max(),\n",
    "            languages_df['Hapax_Ratio'].max()\n",
    "        ],\n",
    "        'Percentile': [\n",
    "            (languages_df['Zipf_Alpha'] < voynich_alpha).mean() * 100,\n",
    "            (languages_df['R_Squared'] < voynich_r2).mean() * 100,\n",
    "            (languages_df['Type_Token_Ratio'] < voynich_ttr).mean() * 100,\n",
    "            (languages_df['Hapax_Ratio'] < voynich_df['Hapax_Ratio'].iloc[0]).mean() * 100\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Create comparative visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Comparative Zipf Analysis: Voynich vs Natural Languages', fontsize=16)\n",
    "    \n",
    "    # Plot data preparation\n",
    "    all_languages = combined_df['Language'].tolist()\n",
    "    all_alphas = combined_df['Zipf_Alpha'].tolist()\n",
    "    all_r2s = combined_df['R_Squared'].tolist()\n",
    "    all_ttrs = combined_df['Type_Token_Ratio'].tolist()\n",
    "    colors = ['red' if lang == 'Voynich Manuscript' else 'skyblue' for lang in all_languages]\n",
    "    \n",
    "    # Plot 1: Alpha distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    natural_alphas = languages_df['Zipf_Alpha'].tolist()\n",
    "    ax1.hist(natural_alphas, bins=15, alpha=0.7, color='skyblue', label='Natural Languages')\n",
    "    ax1.axvline(voynich_alpha, color='red', linestyle='--', linewidth=3, label='Voynich')\n",
    "    ax1.axvline(1.0, color='black', linestyle='-', alpha=0.5, label='Ideal Zipf (α=1.0)')\n",
    "    ax1.set_xlabel('Zipf Alpha (α)')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Zipf Alpha Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Alpha vs R² scatter\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(all_alphas, all_r2s, c=colors, s=80, alpha=0.7, edgecolors='black')\n",
    "    ax2.set_xlabel('Zipf Alpha (α)')\n",
    "    ax2.set_ylabel('R² (Goodness of Fit)')\n",
    "    ax2.set_title('Alpha vs R² Quality')\n",
    "    ax2.annotate('Voynich', (voynich_alpha, voynich_r2), xytext=(10, 10), \n",
    "                textcoords='offset points', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Type-Token Ratio comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    bars = ax3.bar(range(len(all_languages)), all_ttrs, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_xlabel('Languages')\n",
    "    ax3.set_ylabel('Type-Token Ratio')\n",
    "    ax3.set_title('Vocabulary Diversity (Type-Token Ratio)')\n",
    "    ax3.set_xticks(range(len(all_languages)))\n",
    "    ax3.set_xticklabels(all_languages, rotation=45, ha='right')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Similarity ranking (top 10)\n",
    "    ax4 = axes[1, 1]\n",
    "    top_similar = similarity_df.head(10)\n",
    "    sim_names = top_similar['Language'].tolist()\n",
    "    sim_scores = top_similar['Similarity_Score'].tolist()\n",
    "    \n",
    "    bars = ax4.barh(range(len(sim_names)), sim_scores, color='lightcoral', alpha=0.7)\n",
    "    ax4.set_yticks(range(len(sim_names)))\n",
    "    ax4.set_yticklabels(sim_names)\n",
    "    ax4.set_xlabel('Similarity Score (lower = more similar)')\n",
    "    ax4.set_title('Top 10 Languages Most Similar to Voynich')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Combined Results (Voynich + All Languages):\")\n",
    "    display(combined_df)\n",
    "    \n",
    "    print(\"\\nLanguage Similarity to Voynich (All Languages):\")\n",
    "    display(similarity_df)\n",
    "    \n",
    "    print(\"\\nLanguage Fit Quality Rankings (All Languages):\")\n",
    "    display(fit_quality_df)\n",
    "    \n",
    "    print(\"\\nVoynich vs Natural Language Comparison:\")\n",
    "    display(comparison_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f789b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:51:28.505818Z",
     "start_time": "2025-09-03T05:51:26.496685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create comparative plots showing Voynich vs 5 most similar languages on the same axes\n",
    "if similarity_df is not None and language_analyzer.language_results:\n",
    "    \n",
    "    # Get the 5 most similar languages\n",
    "    top_5_similar = similarity_df.head(5)\n",
    "    \n",
    "    # Set up colors for different languages\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    # Create the 4-panel comparative plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Comparative Zipf Analysis: Voynich vs 5 Most Similar Languages', fontsize=16)\n",
    "    \n",
    "    # First plot Voynich (red)\n",
    "    voynich_freq_data = voynich_analyzer.frequency_data\n",
    "    voynich_predicted = voynich_analyzer.zipf_results['predicted_frequencies']\n",
    "    voynich_ranks = voynich_freq_data['rank'].values\n",
    "    voynich_frequencies = voynich_freq_data['frequency'].values\n",
    "    \n",
    "    # Plot 1: Linear scale frequency vs rank\n",
    "    axes[0, 0].scatter(voynich_ranks[:100], voynich_frequencies[:100], \n",
    "                      alpha=0.7, s=30, color=colors[0], label='Voynich')\n",
    "    axes[0, 0].plot(voynich_ranks[:100], voynich_predicted[:100], \n",
    "                   color=colors[0], linewidth=2, linestyle='--', alpha=0.8)\n",
    "    \n",
    "    # Plot 2: Log-log plot\n",
    "    axes[0, 1].loglog(voynich_ranks, voynich_frequencies, 'o', \n",
    "                     alpha=0.7, markersize=3, color=colors[0], label='Voynich')\n",
    "    axes[0, 1].loglog(voynich_ranks, voynich_predicted, \n",
    "                     color=colors[0], linewidth=2, linestyle='--', alpha=0.8)\n",
    "    \n",
    "    # Plot 3: Residuals\n",
    "    voynich_residuals = voynich_frequencies - voynich_predicted\n",
    "    axes[1, 0].scatter(voynich_ranks, voynich_residuals, \n",
    "                      alpha=0.7, s=20, color=colors[0], label='Voynich')\n",
    "    \n",
    "    # Plot 4: Frequency rank product\n",
    "    voynich_products = voynich_freq_data['frequency'] * voynich_freq_data['rank']\n",
    "    axes[1, 1].plot(voynich_ranks[:100], voynich_products[:100], \n",
    "                   color=colors[0], alpha=0.8, linewidth=2, label='Voynich')\n",
    "    \n",
    "    # 5 most similar languages\n",
    "    for i, (_, row) in enumerate(top_5_similar.iterrows(), 1):\n",
    "        language_name = row['Language']\n",
    "        language_key = language_name.lower()\n",
    "        \n",
    "        if language_key in language_analyzer.language_results:\n",
    "            lang_results = language_analyzer.language_results[language_key]\n",
    "            \n",
    "            freq_data = lang_results['frequency_data']\n",
    "            predicted_freq = lang_results['predicted_frequencies']\n",
    "            ranks = freq_data['rank'].values\n",
    "            frequencies = freq_data['frequency'].values\n",
    "            \n",
    "            # Plot 1: Linear scale\n",
    "            axes[0, 0].scatter(ranks[:100], frequencies[:100], \n",
    "                              alpha=0.6, s=25, color=colors[i], label=language_name)\n",
    "            axes[0, 0].plot(ranks[:100], predicted_freq[:100], \n",
    "                           color=colors[i], linewidth=1.5, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Plot 2: Log-log plot\n",
    "            axes[0, 1].loglog(ranks, frequencies, 'o', \n",
    "                             alpha=0.6, markersize=2, color=colors[i], label=language_name)\n",
    "            axes[0, 1].loglog(ranks, predicted_freq, \n",
    "                             color=colors[i], linewidth=1.5, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Plot 3: Residuals\n",
    "            residuals = frequencies - predicted_freq\n",
    "            axes[1, 0].scatter(ranks, residuals, \n",
    "                              alpha=0.6, s=15, color=colors[i], label=language_name)\n",
    "            \n",
    "            # Plot 4: Frequency rank product\n",
    "            products = freq_data['frequency'] * freq_data['rank']\n",
    "            axes[1, 1].plot(ranks[:100], products[:100], \n",
    "                           color=colors[i], alpha=0.7, linewidth=1.5, label=language_name)\n",
    "    \n",
    "    # Customize plots\n",
    "    axes[0, 0].set_xlabel('Rank')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Frequency vs Rank (Linear Scale)')\n",
    "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Rank (log scale)')\n",
    "    axes[0, 1].set_ylabel('Frequency (log scale)')\n",
    "    axes[0, 1].set_title('Log-Log Plot')\n",
    "    axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Rank')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Residuals Plot')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Rank')\n",
    "    axes[1, 1].set_ylabel('Frequency × Rank')\n",
    "    axes[1, 1].set_title('Zipf Product')\n",
    "    axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the comparison metrics\n",
    "    print(\"Languages in comparison:\")\n",
    "    comparison_metrics = pd.DataFrame({\n",
    "        'Language': ['Voynich'] + top_5_similar['Language'].tolist(),\n",
    "        'Zipf_Alpha': [voynich_df['Zipf_Alpha'].iloc[0]] + top_5_similar['Zipf_Alpha'].tolist(),\n",
    "        'R_Squared': [voynich_df['R_Squared'].iloc[0]] + top_5_similar['R_Squared'].tolist(),\n",
    "        'Type_Token_Ratio': [voynich_df['Type_Token_Ratio'].iloc[0]] + top_5_similar['Type_Token_Ratio'].tolist(),\n",
    "        'Similarity_Score': [0.0] + top_5_similar['Similarity_Score'].tolist()\n",
    "    })\n",
    "    \n",
    "    display(comparison_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35d0f7-ea93-489a-844b-c02d65bb3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_comparative_log_log_only(voynich_analyzer, similarity_df, language_analyzer):\n",
    "    \"\"\"\n",
    "    Plot only the log-log plot comparing Voynich Manuscript with the 5 most similar languages.\n",
    "    Requires voynich_analyzer, similarity_df, and language_analyzer with precomputed results.\n",
    "    \"\"\"\n",
    "    if similarity_df is None or not language_analyzer.language_results:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Get the 5 most similar languages\n",
    "    top_5_similar = similarity_df.head(5)\n",
    "    \n",
    "    # Set up colors for different languages\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    # Create the log-log plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot Voynich data\n",
    "    voynich_freq_data = voynich_analyzer.frequency_data\n",
    "    voynich_predicted = voynich_analyzer.zipf_results['predicted_frequencies']\n",
    "    voynich_ranks = voynich_freq_data['rank'].values\n",
    "    voynich_frequencies = voynich_freq_data['frequency'].values\n",
    "    plt.loglog(voynich_ranks, voynich_frequencies, 'o', alpha=0.7, markersize=3, \n",
    "               color=colors[0], label='Voynich')\n",
    "    plt.loglog(voynich_ranks, voynich_predicted, color=colors[0], linewidth=2, \n",
    "               linestyle='--', alpha=0.8)\n",
    "    \n",
    "    # Plot 5 most similar languages\n",
    "    for i, (_, row) in enumerate(top_5_similar.iterrows(), 1):\n",
    "        language_name = row['Language']\n",
    "        language_key = language_name.lower()\n",
    "        \n",
    "        if language_key in language_analyzer.language_results:\n",
    "            lang_results = language_analyzer.language_results[language_key]\n",
    "            freq_data = lang_results['frequency_data']\n",
    "            predicted_freq = lang_results['predicted_frequencies']\n",
    "            ranks = freq_data['rank'].values\n",
    "            frequencies = freq_data['frequency'].values\n",
    "            \n",
    "            plt.loglog(ranks, frequencies, 'o', alpha=0.6, markersize=2, \n",
    "                       color=colors[i], label=language_name)\n",
    "            plt.loglog(ranks, predicted_freq, color=colors[i], linewidth=1.5, \n",
    "                       linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Rank (log scale)')\n",
    "    plt.ylabel('Frequency (log scale)')\n",
    "    plt.title('Log-Log Plot: Voynich vs 5 Most Similar Languages')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_comparative_log_log_only(voynich_analyzer, similarity_df, language_analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be5e23",
   "metadata": {},
   "source": [
    "#### Funtional Words\n",
    "\n",
    "Functional words, or grammatical words, are small but essential elements like articles (the, a), prepositions (in, on, with), pronouns (he, she, it), and conjunctions (and, but, or) that primarily serve grammatical purposes rather than carrying concrete meaning. Unlike *content words* (nouns, verbs, adjectives) that convey the main semantic information, functional words act as the structural glue that binds sentences together and indicates relationships between ideas. \n",
    "\n",
    "In language decipherment, functional words are particularly valuable because they appear with high frequency, follow predictable grammatical patterns, and often have cognates across related languages, making them reliable anchor points for scholars attempting to decode unknown scripts or languages—much like how archaeologists use pottery sherds to date and understand ancient cultures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a369c-02c1-4108-a2f7-57b2ec04b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07a3c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:51:37.206181Z",
     "start_time": "2025-09-03T05:51:28.505818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core entropy functions\n",
    "def conditional_entropy(tokens, word):\n",
    "    if not tokens or word not in tokens:\n",
    "        return 0\n",
    "    word_positions = [i for i, token in enumerate(tokens) if token == word]\n",
    "    if len(word_positions) < 2:\n",
    "        return 0\n",
    "    \n",
    "    contexts = [tokens[pos + 1] for pos in word_positions if pos + 1 < len(tokens)]\n",
    "    if not contexts:\n",
    "        return 0\n",
    "    \n",
    "    context_counter = Counter(contexts)\n",
    "    total_contexts = len(contexts)\n",
    "    entropy = sum(-(count/total_contexts) * math.log2(count/total_contexts) \n",
    "                  for count in context_counter.values())\n",
    "    return entropy\n",
    "\n",
    "# Configuration\n",
    "LANGS = [\"english\", \"arabic\", \"german\", \"hungarian\", \"finnish\", \"romanian\", \"russian\"]\n",
    "TOP_N = 50\n",
    "COLS = [\"voynich\"] + LANGS\n",
    "\n",
    "# Get tokens function\n",
    "def get_tokens_for_language(lang_name):\n",
    "    if lang_name == \"voynich\":\n",
    "        if 'voynich_analyzer' in globals() and hasattr(voynich_analyzer, 'tokens'):\n",
    "            return voynich_analyzer.tokens\n",
    "    else:\n",
    "        if 'language_analyzer' in globals() and hasattr(language_analyzer, 'language_results'):\n",
    "            lang_key = lang_name.lower()\n",
    "            if lang_key in language_analyzer.language_results:\n",
    "                freq_data = language_analyzer.language_results[lang_key]['frequency_data']\n",
    "                tokens = []\n",
    "                for _, row in freq_data.iterrows():\n",
    "                    tokens.extend([row['token']] * row['frequency'])\n",
    "                return tokens\n",
    "    return []\n",
    "\n",
    "# Load stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stopsets = {}\n",
    "for lang in LANGS:\n",
    "    try:\n",
    "        stopsets[lang] = set(stopwords.words(lang))\n",
    "    except:\n",
    "        stopsets[lang] = set()\n",
    "stopsets[\"voynich\"] = set()\n",
    "\n",
    "# Calculate entropy data and build frequency lists\n",
    "entropy_analysis = {}\n",
    "top_lists = {}\n",
    "\n",
    "for col in COLS:\n",
    "    tokens = get_tokens_for_language(col)\n",
    "    if tokens:\n",
    "        total = len(tokens)\n",
    "        counter = Counter(tokens)\n",
    "        most_common = counter.most_common(TOP_N)\n",
    "        \n",
    "        # Entropy analysis\n",
    "        word_entropies = {word: conditional_entropy(tokens, word) for word, count in most_common}\n",
    "        avg_cond_entropy = np.mean(list(word_entropies.values())) if word_entropies else 0\n",
    "        \n",
    "        entropy_analysis[col] = {\n",
    "            'word_entropies': word_entropies,\n",
    "            'avg_conditional_entropy': avg_cond_entropy\n",
    "        }\n",
    "        \n",
    "        # Frequency lists\n",
    "        top_lists[col] = [(word, (count/total)*100) for word, count in most_common]\n",
    "    else:\n",
    "        entropy_analysis[col] = {'word_entropies': {}, 'avg_conditional_entropy': 0}\n",
    "        top_lists[col] = []\n",
    "\n",
    "# Build DataFrame\n",
    "data = {}\n",
    "for col in COLS:\n",
    "    rows = []\n",
    "    for i in range(TOP_N):\n",
    "        if i < len(top_lists.get(col, [])):\n",
    "            word, pct = top_lists[col][i]\n",
    "            rows.append(f\"{word}\\n{pct:.3f}%\")\n",
    "        else:\n",
    "            rows.append(\"\")\n",
    "    data[col] = rows\n",
    "\n",
    "df_result = pd.DataFrame(data, index=pd.RangeIndex(1, TOP_N+1, name=\"rank\"))\n",
    "\n",
    "# Styling function\n",
    "def make_entropy_styles(df):\n",
    "    styles = pd.DataFrame(\"\", index=df.index, columns=df.columns)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col not in entropy_analysis or not entropy_analysis[col]['word_entropies']:\n",
    "            continue\n",
    "        \n",
    "        word_entropies = entropy_analysis[col]['word_entropies']\n",
    "        avg_entropy = entropy_analysis[col]['avg_conditional_entropy']\n",
    "        sset = stopsets.get(col, set())\n",
    "        \n",
    "        for idx, cell in df[col].items():\n",
    "            if cell and cell != \"\":\n",
    "                word = str(cell).split(\"\\n\")[0]\n",
    "                \n",
    "                if word in word_entropies:\n",
    "                    word_entropy = word_entropies[word]\n",
    "                    is_stopword = word in sset or word.lower() in sset\n",
    "                    \n",
    "                    if word_entropy < avg_entropy:  # Functional words\n",
    "                        if col != \"voynich\" and is_stopword:\n",
    "                            styles.at[idx, col] = \"background-color: #bbdefb\"\n",
    "                        else:\n",
    "                            styles.at[idx, col] = \"background-color: #ffcdd2\" \n",
    "                    else:  # Content words\n",
    "                        styles.at[idx, col] = \"background-color: #fff176\" \n",
    "    \n",
    "    return styles\n",
    "\n",
    "# Apply styling and display\n",
    "styled_result = df_result.style.apply(lambda _: make_entropy_styles(df_result), axis=None)\n",
    "\n",
    "print(\"Functional Word Analysis with Entropy\")\n",
    "print(\"🔴 Light Red: Functional words  🟨 Light Yellow: Content words  🔵 Light Blue: Functional + stopwords\")\n",
    "print()\n",
    "display(styled_result)\n",
    "\n",
    "# Classification summary\n",
    "if 'voynich' in entropy_analysis and entropy_analysis['voynich']['word_entropies']:\n",
    "    word_entropies = entropy_analysis['voynich']['word_entropies']\n",
    "    avg_entropy = entropy_analysis['voynich']['avg_conditional_entropy']\n",
    "    \n",
    "    structural_words = [word for word, entropy in word_entropies.items() if entropy < avg_entropy]\n",
    "    content_words = [word for word, entropy in word_entropies.items() if entropy >= avg_entropy]\n",
    "else:\n",
    "    print(\"\\nNo Voynich entropy data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0218123-4d80-4943-9b98-061b98561666",
   "metadata": {},
   "source": [
    "**Note.:** Topical Words can be both frequent and have a low entropy in a text, very similar to a functional word. We can see that with words like \"philosophy\", \"culture\", \"history\" and \"information\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f616420",
   "metadata": {},
   "source": [
    "#### Entropy Analysis\n",
    "\n",
    "Entropy measures the amount of information or unpredictability in a dataset by calculating how evenly distributed different elements are within a system. In linguistic analysis, entropy can reveal crucial structural properties of both known and unknown languages by quantifying the distribution patterns of letters, phonemes, or words. \n",
    "\n",
    "Languages exhibit characteristic entropy signatures: for instance, functional words create low-entropy patterns due to their high frequency and limited variety, while content words generate higher entropy through their diverse vocabulary. When applied to undeciphered texts, entropy analysis can identify writing system types (alphabetic vs. logographic), detect word boundaries, and reveal whether symbols represent individual sounds or complete concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447ce5d-c153-4edc-be20-2f6dad98c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09abec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:51:37.230069Z",
     "start_time": "2025-09-03T05:51:37.208670Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a list of languages to analyze\n",
    "LANGS = [\"voynich\", \"english\", \"spanish\", \"french\", \"german\", \"russian\", \"turkish\", \n",
    "         \"finnish\", \"hungarian\", \"basque\", \"polish\", \"bulgarian\", \"greek\", \"albanian\", \n",
    "         \"romanian\", \"irish\", \"maltese\", \"arabic\", \"hebrew\", \"latin\"]\n",
    "\n",
    "# Retrieve tokens\n",
    "def get_tokens_for_language(lang_name):\n",
    "    if lang_name == \"voynich\":\n",
    "        if 'voynich_analyzer' in globals() and hasattr(voynich_analyzer, 'tokens'):\n",
    "            return voynich_analyzer.tokens\n",
    "    else:\n",
    "        if 'language_analyzer' in globals() and hasattr(language_analyzer, 'language_results'):\n",
    "            lang_key = lang_name.lower() \n",
    "            if lang_key in language_analyzer.language_results:\n",
    "                freq_data = language_analyzer.language_results[lang_key]['frequency_data']\n",
    "                tokens = []\n",
    "                for _, row in freq_data.iterrows():\n",
    "                    tokens.extend([row['token']] * row['frequency'])\n",
    "                return tokens\n",
    "    return []\n",
    "\n",
    "# Calculate n-gram entropy\n",
    "def calculate_ngram_entropy(tokens, n):\n",
    "    if not tokens or len(tokens) < n:\n",
    "        return 0\n",
    "    \n",
    "    # Generate n-grams\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    # Return 0 if no n-grams are generated\n",
    "    if not ngrams:\n",
    "        return 0\n",
    "    \n",
    "    # Count frequency of each n-gram\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    entropy = 0\n",
    "    \n",
    "    # Calculate entropy using Shannon's formula: -sum(p * log2(p))\n",
    "    for count in counter.values():\n",
    "        p = count / total  # Probability of the n-gram\n",
    "        if p > 0:\n",
    "            entropy -= p * math.log2(p)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Calculate cross-entropy between source and target token sequences\n",
    "def calculate_cross_entropy(source_tokens, target_tokens, n=2):\n",
    "    if not source_tokens or not target_tokens:\n",
    "        return float('inf')\n",
    "    \n",
    "    source_ngrams = []\n",
    "    for i in range(len(source_tokens) - n + 1):\n",
    "        source_ngrams.append(tuple(source_tokens[i:i+n]))\n",
    "    \n",
    "    if not source_ngrams:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Count frequency of source n-grams\n",
    "    source_counter = Counter(source_ngrams)\n",
    "    source_total = len(source_ngrams)\n",
    "    \n",
    "    source_probs = {}\n",
    "    vocab_size = len(set(source_ngrams))\n",
    "    smoothing = 1.0\n",
    "    \n",
    "    for ngram in source_counter:\n",
    "        source_probs[ngram] = (source_counter[ngram] + smoothing) / (source_total + vocab_size * smoothing)\n",
    "    \n",
    "    # Default probability for unseen n-grams\n",
    "    default_prob = smoothing / (source_total + vocab_size * smoothing)\n",
    "    \n",
    "    # Generate n-grams from target tokens\n",
    "    target_ngrams = []\n",
    "    for i in range(len(target_tokens) - n + 1):\n",
    "        target_ngrams.append(tuple(target_tokens[i:i+n]))\n",
    "    \n",
    "    if not target_ngrams:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Calculate cross-entropy using source probabilities\n",
    "    cross_entropy = 0\n",
    "    for ngram in target_ngrams:\n",
    "        prob = source_probs.get(ngram, default_prob)\n",
    "        if prob > 0:\n",
    "            cross_entropy -= math.log2(prob)\n",
    "    \n",
    "    cross_entropy /= len(target_ngrams)\n",
    "    return cross_entropy\n",
    "\n",
    "# Calculate character transitions within tokens, filtering out non-letter characters (punctuation, etc.), but keeping letters with accents/diacritics\n",
    "def calculate_character_transitions(tokens):\n",
    "    # Initialize defaultdict to store transitions as counters\n",
    "    transitions = defaultdict(Counter)\n",
    "    for token in tokens:\n",
    "        if len(token) > 1:  # Only process tokens with at least 2 characters\n",
    "            for i in range(len(token) - 1):\n",
    "                char1 = token[i]\n",
    "                char2 = token[i+1]  # Current and next character\n",
    "                # Only count transitions between alphabetic characters (includes accented letters, excludes punctuation/digits)\n",
    "                if char1.isalpha() and char2.isalpha():\n",
    "                    transitions[char1][char2] += 1\n",
    "    return transitions\n",
    "\n",
    "# Calculate entropy of character transitions\n",
    "def calculate_transition_entropy(transitions):\n",
    "    if not transitions:\n",
    "        return 0\n",
    "    \n",
    "    total_entropy = 0\n",
    "    char_count = 0\n",
    "    \n",
    "    for char1, next_chars in transitions.items():\n",
    "        if not next_chars:\n",
    "            continue\n",
    "            \n",
    "        total_transitions = sum(next_chars.values())  # Total transitions from char1\n",
    "        char_entropy = 0\n",
    "        \n",
    "        # Calculate entropy for transitions from char1\n",
    "        for count in next_chars.values():\n",
    "            p = count / total_transitions\n",
    "            if p > 0:\n",
    "                char_entropy -= p * math.log2(p)\n",
    "        \n",
    "        total_entropy += char_entropy\n",
    "        char_count += 1\n",
    "    \n",
    "    return total_entropy / char_count if char_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e6ed2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:51:46.901445Z",
     "start_time": "2025-09-03T05:51:37.232071Z"
    }
   },
   "outputs": [],
   "source": [
    "# N-gram Entropy\n",
    "def analyze_ngram_progression():\n",
    "    print(\"=== N-GRAM ENTROPY PROGRESSION ANALYSIS ===\")\n",
    "    \n",
    "    results = {}\n",
    "    n_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # N-gram sizes to analyze\n",
    "    \n",
    "    # Iterate through each language\n",
    "    for lang in LANGS:\n",
    "        tokens = get_tokens_for_language(lang)\n",
    "        if tokens:\n",
    "            lang_results = {}\n",
    "            # Calculate entropy for each n-gram size\n",
    "            for n in n_values:\n",
    "                entropy = calculate_ngram_entropy(tokens, n)\n",
    "                lang_results[f\"{n}-gram\"] = entropy\n",
    "            \n",
    "            # Calculate entropy values and reduction rates\n",
    "            entropies = [lang_results[f\"{n}-gram\"] for n in n_values]\n",
    "            reduction_rate = []\n",
    "            for i in range(1, len(entropies)):\n",
    "                if entropies[i-1] > 0:\n",
    "                    reduction = (entropies[i-1] - entropies[i]) / entropies[i-1]\n",
    "                    reduction_rate.append(reduction)\n",
    "                else:\n",
    "                    reduction_rate.append(0)\n",
    "            \n",
    "            lang_results['avg_reduction_rate'] = np.mean(reduction_rate)\n",
    "            results[lang] = lang_results\n",
    "    \n",
    "    # Create df ordered by reduction rate similarity to Voynich\n",
    "    if 'voynich' in results:\n",
    "        voynich_rate = results['voynich']['avg_reduction_rate']\n",
    "        \n",
    "        # Sort languages by similarity to Voynich reduction rate\n",
    "        sorted_langs = sorted(results.keys(), \n",
    "                             key=lambda x: abs(results[x]['avg_reduction_rate'] - voynich_rate))\n",
    "        \n",
    "        df_ngram = pd.DataFrame({lang: results[lang] for lang in sorted_langs}).T\n",
    "    else:\n",
    "        df_ngram = pd.DataFrame(results).T\n",
    "    \n",
    "    df_ngram = df_ngram.round(3)  # Round values to 3 decimal places\n",
    "    \n",
    "    print(\"N-gram Entropy Values (ordered by similarity to Voynich):\")\n",
    "    display(df_ngram)\n",
    "    \n",
    "    # Plot n-gram entropy progression\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for lang in results.keys():\n",
    "        entropies = [results[lang][f\"{n}-gram\"] for n in n_values]\n",
    "        if lang == 'voynich':\n",
    "            # Highlight Voynich with distinct style\n",
    "            plt.plot(n_values, entropies, marker='o', label='Voynich', \n",
    "                    linewidth=3, color='red')\n",
    "        else:\n",
    "            # Plot other languages with gray lines\n",
    "            plt.plot(n_values, entropies, marker='o', \n",
    "                    linewidth=1.5, color='gray', alpha=0.7)\n",
    "    \n",
    "    # Add \"Other languages\" to legend\n",
    "    plt.plot([], [], color='gray', linewidth=1.5, alpha=0.7, label='Other languages')\n",
    "    \n",
    "    plt.xlabel('N-gram Size')\n",
    "    plt.ylabel('Entropy (bits)')\n",
    "    plt.title('N-gram Entropy Progression: Voynich vs Other Languages')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(n_values)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    if 'voynich' in results:\n",
    "        voynich_rate = results['voynich']['avg_reduction_rate']\n",
    "        print(f\"\\nVoynich entropy reduction rate: {voynich_rate:.3f}\")\n",
    "        \n",
    "        other_rates = [(lang, data['avg_reduction_rate']) \n",
    "                      for lang, data in results.items() if lang != 'voynich']\n",
    "        other_rates.sort(key=lambda x: abs(x[1] - voynich_rate))\n",
    "        \n",
    "        print(\"Languages with most similar entropy reduction patterns:\")\n",
    "        for i, (lang, rate) in enumerate(other_rates[:5]):\n",
    "            diff = abs(rate - voynich_rate)\n",
    "            print(f\"{i+1}. {lang}: {rate:.3f} (diff: {diff:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "ngram_results = analyze_ngram_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcbb1c-6461-435e-ac4f-48186c4901cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def analyze_ngram_progression():\n",
    "    print(\"=== N-GRAM ENTROPY PROGRESSION ANALYSIS ===\")\n",
    "    \n",
    "    results = {}\n",
    "    n_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # N-gram sizes to analyze\n",
    "    \n",
    "    # Iterate through each language\n",
    "    for lang in LANGS:\n",
    "        tokens = get_tokens_for_language(lang)\n",
    "        if tokens:\n",
    "            # For non-Voynich languages, convert tokens to character-level\n",
    "            if lang != 'voynich':\n",
    "                # Join tokens (assuming they are words) and split into characters\n",
    "                text = ''.join(tokens) if isinstance(tokens, list) else tokens\n",
    "                tokens = list(text.lower())  # Convert to lowercase characters\n",
    "            lang_results = {}\n",
    "            # Calculate entropy for each n-gram size\n",
    "            for n in n_values:\n",
    "                entropy = calculate_ngram_entropy(tokens, n)\n",
    "                lang_results[f\"{n}-gram\"] = entropy\n",
    "            \n",
    "            # Calculate entropy values and reduction rates\n",
    "            entropies = [lang_results[f\"{n}-gram\"] for n in n_values]\n",
    "            reduction_rate = []\n",
    "            for i in range(1, len(entropies)):\n",
    "                if entropies[i-1] > 0:\n",
    "                    reduction = (entropies[i-1] - entropies[i]) / entropies[i-1]\n",
    "                    reduction_rate.append(reduction)\n",
    "                else:\n",
    "                    reduction_rate.append(0)\n",
    "            \n",
    "            lang_results['avg_reduction_rate'] = np.mean(reduction_rate)\n",
    "            results[lang] = lang_results\n",
    "    \n",
    "    # Create df ordered by reduction rate similarity to Voynich\n",
    "    if 'voynich' in results:\n",
    "        voynich_rate = results['voynich']['avg_reduction_rate']\n",
    "        \n",
    "        # Sort languages by similarity to Voynich reduction rate\n",
    "        sorted_langs = sorted(results.keys(), \n",
    "                             key=lambda x: abs(results[x]['avg_reduction_rate'] - voynich_rate))\n",
    "        \n",
    "        df_ngram = pd.DataFrame({lang: results[lang] for lang in sorted_langs}).T\n",
    "    else:\n",
    "        df_ngram = pd.DataFrame(results).T\n",
    "    \n",
    "    df_ngram = df_ngram.round(3)  # Round values to 3 decimal places\n",
    "    \n",
    "    print(\"N-gram Entropy Values (ordered by similarity to Voynich):\")\n",
    "    display(df_ngram)\n",
    "    \n",
    "    # Plot n-gram entropy progression\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for lang in results.keys():\n",
    "        entropies = [results[lang][f\"{n}-gram\"] for n in n_values]\n",
    "        if lang == 'voynich':\n",
    "            # Highlight Voynich with distinct style\n",
    "            plt.plot(n_values, entropies, marker='o', label='Voynich', \n",
    "                    linewidth=3, color='red')\n",
    "        else:\n",
    "            # Plot other languages with gray lines\n",
    "            plt.plot(n_values, entropies, marker='o', \n",
    "                    linewidth=1.5, color='gray', alpha=0.7)\n",
    "    \n",
    "    # Add \"Other languages\" to legend\n",
    "    plt.plot([], [], color='gray', linewidth=1.5, alpha=0.7, label='Other languages')\n",
    "    \n",
    "    plt.xlabel('N-gram Size')\n",
    "    plt.ylabel('Entropy (bits)')\n",
    "    plt.title('N-gram Entropy Progression: Voynich vs Other Languages (Character-Level)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(n_values)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    if 'voynich' in results:\n",
    "        voynich_rate = results['voynich']['avg_reduction_rate']\n",
    "        print(f\"\\nVoynich entropy reduction rate: {voynich_rate:.3f}\")\n",
    "        \n",
    "        other_rates = [(lang, data['avg_reduction_rate']) \n",
    "                      for lang, data in results.items() if lang != 'voynich']\n",
    "        other_rates.sort(key=lambda x: abs(x[1] - voynich_rate))\n",
    "        \n",
    "        print(\"Languages with most similar entropy reduction patterns:\")\n",
    "        for i, (lang, rate) in enumerate(other_rates[:5]):\n",
    "            diff = abs(rate - voynich_rate)\n",
    "            print(f\"{i+1}. {lang}: {rate:.3f} (diff: {diff:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "ngram_results = analyze_ngram_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5a595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:51:57.037519Z",
     "start_time": "2025-09-03T05:51:46.901445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Symbol Transition Entropy\n",
    "def analyze_symbol_transitions():\n",
    "    print(\"\\n=== SYMBOL TRANSITION ENTROPY ANALYSIS ===\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Iterate through each language\n",
    "    for lang in LANGS:\n",
    "        tokens = get_tokens_for_language(lang)\n",
    "        if tokens:\n",
    "            # Calculate character transitions and related metrics\n",
    "            transitions = calculate_character_transitions(tokens)\n",
    "            \n",
    "            transition_entropy = calculate_transition_entropy(transitions)\n",
    "            num_unique_chars = len(transitions)\n",
    "            avg_transitions_per_char = np.mean([len(next_chars) for next_chars in transitions.values()]) if transitions else 0\n",
    "            \n",
    "            results[lang] = {\n",
    "                'transition_entropy': transition_entropy,\n",
    "                'unique_characters': num_unique_chars,\n",
    "                'avg_transitions_per_char': avg_transitions_per_char,\n",
    "                'transitions': transitions\n",
    "            }\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = {\n",
    "        lang: {\n",
    "            'Transition_Entropy': data['transition_entropy'],\n",
    "            'Unique_Characters': data['unique_characters'],\n",
    "            'Avg_Transitions_per_Char': data['avg_transitions_per_char']\n",
    "        }\n",
    "        for lang, data in results.items()\n",
    "    }\n",
    "    \n",
    "    df_transitions = pd.DataFrame(summary_data).T.round(3)\n",
    "    \n",
    "    print(\"Character Transition Analysis:\")\n",
    "    display(df_transitions)\n",
    "    \n",
    "    # Find languages with transition entropy closest to Voynich\n",
    "    if 'voynich' in results:\n",
    "        voynich_entropy = results['voynich']['transition_entropy']\n",
    "        \n",
    "        similarities = []\n",
    "        for lang, data in results.items():\n",
    "            if lang != 'voynich':\n",
    "                entropy_diff = abs(data['transition_entropy'] - voynich_entropy)\n",
    "                similarities.append((lang, entropy_diff))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"Languages with most similar character transition patterns to Voynich:\")\n",
    "        for i, (lang, diff) in enumerate(similarities[:5]):\n",
    "            print(f\"{i+1}. {lang}: entropy diff = {diff:.5f}\")\n",
    "    \n",
    "    # Plot scatter of character diversity vs transition entropy\n",
    "    if results:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        langs = list(results.keys())\n",
    "        char_counts = [results[lang]['unique_characters'] for lang in langs]\n",
    "        entropies = [results[lang]['transition_entropy'] for lang in langs]\n",
    "        colors = ['red' if lang == 'voynich' else 'gray' for lang in langs]\n",
    "        \n",
    "        plt.scatter(char_counts, entropies, c=colors, s=100, alpha=0.7)\n",
    "        \n",
    "        # Annotate each point with language name\n",
    "        for i, lang in enumerate(langs):\n",
    "            plt.annotate(lang, (char_counts[i], entropies[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        plt.xlabel('Number of Unique Characters')\n",
    "        plt.ylabel('Transition Entropy')\n",
    "        plt.title('Character Diversity vs Transition Complexity')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "transition_results = analyze_symbol_transitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb12614",
   "metadata": {},
   "source": [
    "#### Word Grammar\n",
    "\n",
    "This section implements Jorge Stolfi's grammatical analysis of Voynichese words, based on his three-layer structural model. Stolfi's grammar treats each word as having a nested density profile with a \"core\" layer (gallows letters like t, p, k, f), a \"mantle\" layer (bench letters like ch, sh and e-groups), and a \"crust\" layer (dealers, terminals, and initial letters). The model uses statistical frequency data from the actual manuscript to validate word structures and assign probability scores.\n",
    "\n",
    "The implementation tokenizes words into grammatical units, calculates density profiles, and checks structural constraints like the tendency for density to peak at the core and decline toward word edges. While this represents a simplified version of Stolfi's complete context-free grammar, it captures the essential insight that Voynichese exhibits systematic internal structure rather than random letter sequences, making it a valuable tool for computational analysis of the manuscript's linguistic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab99ac-570d-40e7-a857-c3c3eb70e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2927130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T05:52:01.448872Z",
     "start_time": "2025-09-03T05:51:57.041053Z"
    }
   },
   "outputs": [],
   "source": [
    "class VoynichGrammarValidator:\n",
    "    def __init__(self):\n",
    "        # Define character sets based on Stolfi's classifications from the grammar document\n",
    "        self.GALLOWS = {\"k\", \"t\", \"p\", \"f\"}  # Core letters: single gallows\n",
    "        self.PEDESTALS = {\"cth\", \"cph\", \"ckh\", \"cfh\"}  # Core pedestal combinations (c + gallow + h)\n",
    "        self.BENCHES = {\"ch\", \"sh\", \"ee\"}  # Mantle benches\n",
    "        self.ISOLATED_E = {\"e\"}  # Isolated 'e' treated as mantle\n",
    "        self.STAVES = {\"i\", \"l\", \"r\", \"s\"}  # Crust staves (dealers like r, s)\n",
    "        self.LOOPS = {\"a\", \"o\", \"y\"}  # Circles/modifiers (ignored in density)\n",
    "        self.LEGS = {\"d\"}  # Crust leg (dealer d)\n",
    "        self.JIBS = {\"q\"}  # Crust jib (initial q)\n",
    "        self.OTHERS = {\"m\", \"g\", \"n\", \"x\"}  # Other crust letters (mostly finals)\n",
    "\n",
    "        # Density levels as per the layered model (higher density inward)\n",
    "        self.DENSITY_CORE = 3  # Gallows/pedestals\n",
    "        self.DENSITY_MANTLE = 2  # Benches/isolated e\n",
    "        self.DENSITY_CRUST = 1  # Dealers/initials/finals\n",
    "        self.DENSITY_NONE = 0  # Undefined or errors\n",
    "\n",
    "    # Load and clean the Voynich manuscript text\n",
    "    def load_and_preprocess_text(self, filepath, treat_commas_as_spaces=True):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            eva_text_segments = []\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "\n",
    "                # Skip empty lines and comment lines starting with '#'\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "\n",
    "                # Match lines with locus identifiers like <f1r> and extract the EVA part\n",
    "                match = re.match(r'<f[^>]+>\\s*(.*)', line)\n",
    "                if match:\n",
    "                    eva_part = match.group(1).strip()\n",
    "                    if eva_part:\n",
    "                        processed_eva = self.process_eva_text(eva_part, treat_commas_as_spaces)\n",
    "                        if processed_eva:\n",
    "                            eva_text_segments.append(processed_eva)\n",
    "\n",
    "            # Join all segments into a single text string\n",
    "            final_text = ' '.join(eva_text_segments)\n",
    "            return final_text\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    # Process a single EVA text segment\n",
    "    def process_eva_text(self, eva_text, treat_commas_as_spaces=True):\n",
    "        if treat_commas_as_spaces:\n",
    "            eva_text = eva_text.replace(',', ' ')\n",
    "\n",
    "        # Remove special characters (keep letters and spaces)\n",
    "        eva_text = re.sub(r'[^\\w\\s]', ' ', eva_text)\n",
    "        # Remove digits\n",
    "        eva_text = re.sub(r'\\d', '', eva_text)\n",
    "        # Normalize multiple spaces to single\n",
    "        eva_text = re.sub(r'\\s+', ' ', eva_text)\n",
    "        \n",
    "\n",
    "        return eva_text.strip()\n",
    "\n",
    "    # Assign density level to a token based on its category.\n",
    "    def get_character_density(self, token):\n",
    "        if token in self.GALLOWS or token in self.PEDESTALS:\n",
    "            return self.DENSITY_CORE\n",
    "        elif token in self.BENCHES or token in self.ISOLATED_E:\n",
    "            return self.DENSITY_MANTLE\n",
    "        elif token in (self.STAVES | self.LEGS | self.JIBS | self.OTHERS):\n",
    "            return self.DENSITY_CRUST\n",
    "        else:\n",
    "            return self.DENSITY_NONE\n",
    "\n",
    "    # Tokenize a Voynichese word into grammatical units\n",
    "    def tokenize_word(self, word_string):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        word_string = word_string.lower().strip()\n",
    "\n",
    "        while i < len(word_string):\n",
    "            # Check for 3-char pedestals: 'c' + gallow + 'h'\n",
    "            if i + 2 < len(word_string) and word_string[i] == 'c' and word_string[i+1] in self.GALLOWS and word_string[i+2] == 'h':\n",
    "                tokens.append(word_string[i:i+3])\n",
    "                i += 3\n",
    "                continue\n",
    "\n",
    "            # Check for 2-char benches: 'ch', 'sh'\n",
    "            if i + 1 < len(word_string):\n",
    "                two_char = word_string[i:i+2]\n",
    "                if two_char in self.BENCHES:\n",
    "                    tokens.append(two_char)\n",
    "                    i += 2\n",
    "                    continue\n",
    "\n",
    "            # Check for 'ee' (mantle bench)\n",
    "            if i + 1 < len(word_string) and word_string[i:i+2] == 'ee':\n",
    "                tokens.append('ee')\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            # Fall back to single character tokens (e.g., 'e', circles, crust letters)\n",
    "            tokens.append(word_string[i])\n",
    "            i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Compute the density profile of the word, excluding circle modifiers (a, o, y)\n",
    "    def calculate_density_profile(self, tokens):\n",
    "        density_profile = []\n",
    "        for token in tokens:\n",
    "            if token in self.LOOPS:\n",
    "                continue  # Circles are ignored in density profiles as per grammar\n",
    "            density = self.get_character_density(token)\n",
    "            density_profile.append(density)  # Append even if 0 for error detection\n",
    "        return density_profile\n",
    "\n",
    "    # Identify the core: position of the maximum density in the profile.\n",
    "    def find_word_core(self, density_profile):\n",
    "        if not density_profile:\n",
    "            return -1, 0\n",
    "\n",
    "        max_density = max(density_profile)\n",
    "        core_index = density_profile.index(max_density)\n",
    "        return core_index, max_density\n",
    "\n",
    "    # Verify unimodal density profile: non-decreasing to core, non-increasing after.\n",
    "    def check_unimodal_density(self, density_profile, core_index):\n",
    "        errors = []\n",
    "\n",
    "        # Prefix: non-decreasing (rises or flat to core)\n",
    "        for i in range(core_index):\n",
    "            if density_profile[i] > density_profile[i + 1]:\n",
    "                errors.append(\"density_decreases_before_core\")\n",
    "                break\n",
    "\n",
    "        # Suffix: non-increasing (falls or flat after core)\n",
    "        for i in range(core_index, len(density_profile) - 1):\n",
    "            if density_profile[i] < density_profile[i + 1]:\n",
    "                errors.append(\"density_increases_after_core\")\n",
    "                break\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def check_gallows_rules(self, tokens):\n",
    "        \"\"\"\n",
    "        Check gallows/pedestal rules:\n",
    "        - Up to 2 per word (in core); >2 is abnormal (multi-gallows).\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        gallows_count = sum(1 for token in tokens if token in self.GALLOWS or token in self.PEDESTALS)\n",
    "\n",
    "        if gallows_count > 2:\n",
    "            errors.append(\"multiple_gallows\")\n",
    "\n",
    "        return errors\n",
    "        \n",
    "    # Ensure 'q' is only at the word start (initial position).\n",
    "    def check_q_placement(self, tokens):\n",
    "        errors = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == 'q' and i != 0:\n",
    "                errors.append(\"q_misplaced\")\n",
    "                break\n",
    "\n",
    "        return errors\n",
    "\n",
    "    # Prohibit adjacent circles (no aa, ao, etc.). \n",
    "    def check_adjacent_circles(self, tokens):\n",
    "        errors = []\n",
    "        for i in range(len(tokens) - 1):\n",
    "            if tokens[i] in self.LOOPS and tokens[i+1] in self.LOOPS:\n",
    "                errors.append(\"adjacent_circles\")\n",
    "                break\n",
    "        return errors\n",
    "\n",
    "    # Ensure 'y' is mostly word-final; error if not at end.\n",
    "    def check_y_placement(self, tokens):\n",
    "        errors = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == 'y' and i != len(tokens) - 1:\n",
    "                errors.append(\"y_misplaced\")\n",
    "                break\n",
    "        return errors\n",
    "        \n",
    "    # Validate single word\n",
    "    def validate_word(self, word_string):\n",
    "\n",
    "        if not word_string or len(word_string.strip()) == 0:\n",
    "            return False, [\"empty_word\"], {}\n",
    "\n",
    "        # Tokenize the word\n",
    "        tokens = self.tokenize_word(word_string)\n",
    "        if not tokens:\n",
    "            return False, [\"tokenization_failed\"], {}\n",
    "\n",
    "        # Check for no adjacent circles\n",
    "        adjacent_errors = self.check_adjacent_circles(tokens)\n",
    "\n",
    "        # Check 'y' is final\n",
    "        y_errors = self.check_y_placement(tokens)\n",
    "\n",
    "        # Build density profile (excludes circles)\n",
    "        density_profile = self.calculate_density_profile(tokens)\n",
    "        core_index, max_density = self.find_word_core(density_profile)\n",
    "\n",
    "        # Aggregate errors\n",
    "        errors = adjacent_errors + y_errors\n",
    "\n",
    "        # Add unimodal check errors\n",
    "        unimodal_errors = self.check_unimodal_density(density_profile, core_index)\n",
    "        errors.extend(unimodal_errors)\n",
    "\n",
    "        # Add gallows errors\n",
    "        gallows_errors = self.check_gallows_rules(tokens)\n",
    "        errors.extend(gallows_errors)\n",
    "\n",
    "        # Add q placement errors\n",
    "        q_errors = self.check_q_placement(tokens)\n",
    "        errors.extend(q_errors)\n",
    "\n",
    "        # Detect undefined tokens (non-loops with density 0, e.g., standalone 'c')\n",
    "        undefined_count = sum(1 for t in tokens if self.get_character_density(t) == self.DENSITY_NONE and t not in self.LOOPS)\n",
    "        if undefined_count > 0:\n",
    "            errors.append(\"undefined_tokens\")\n",
    "\n",
    "        # Prepare analysis data\n",
    "        analysis_data = {\n",
    "            'tokens': tokens,\n",
    "            'density_profile': density_profile,\n",
    "            'core_index': core_index,\n",
    "            'max_density': max_density,\n",
    "            'word_length': len(tokens),  # Token count as length\n",
    "            'token_count': len(tokens),\n",
    "            'gallows_count': sum(1 for t in tokens if t in self.GALLOWS or t in self.PEDESTALS),\n",
    "            'benches_count': sum(1 for t in tokens if t in self.BENCHES or t in self.ISOLATED_E),\n",
    "            'loops_count': sum(1 for t in tokens if t in self.LOOPS)\n",
    "        }\n",
    "\n",
    "        is_valid = len(errors) == 0\n",
    "\n",
    "        # Classify based on validity and structure\n",
    "        if not is_valid:\n",
    "            if \"multiple_gallows\" in errors or (density_profile and density_profile.count(self.DENSITY_CORE) > 2):\n",
    "                analysis_data['classification'] = 'multi_core'\n",
    "            elif \"adjacent_circles\" in errors or \"undefined_tokens\" in errors:\n",
    "                analysis_data['classification'] = 'weird'\n",
    "            else:\n",
    "                analysis_data['classification'] = 'other_abnormal'\n",
    "        else:\n",
    "            if analysis_data.get('max_density', 0) >= 3:\n",
    "                analysis_data['classification'] = 'highly_structured'\n",
    "            elif analysis_data.get('max_density', 0) >= 2:\n",
    "                analysis_data['classification'] = 'moderately_structured'\n",
    "            else:\n",
    "                analysis_data['classification'] = 'lowly_structured'\n",
    "\n",
    "        return is_valid, errors, analysis_data\n",
    "\n",
    "    def analyze_voynich_text(self, filepath, treat_commas_as_spaces=True, min_word_length=2):\n",
    "        \"\"\"\n",
    "        Analyze the full Voynich text:\n",
    "        - Load text, split into words, get unique words and frequencies.\n",
    "        - Validate each unique word, compile into DataFrame.\n",
    "        - Compute consolidated stats.\n",
    "        - Focuses on unique words for efficiency.\n",
    "        \"\"\"\n",
    "        # Load and preprocess the text\n",
    "        text = self.load_and_preprocess_text(filepath, treat_commas_as_spaces)\n",
    "\n",
    "        if not text:\n",
    "            return None, None\n",
    "\n",
    "        # Split into words using delimiters\n",
    "        all_words = re.split(r'[,.\\s=\\-]+', text)\n",
    "        all_words = [w.strip() for w in all_words if w.strip()]\n",
    "\n",
    "        # Count frequencies\n",
    "        word_frequencies = Counter(all_words)\n",
    "\n",
    "        # Filter by min length\n",
    "        filtered_word_frequencies = {word: freq for word, freq in word_frequencies.items()\n",
    "                                     if len(word) >= min_word_length}\n",
    "\n",
    "        unique_words = list(filtered_word_frequencies.keys())\n",
    "\n",
    "        # Print summary stats\n",
    "        total_instances = len(all_words)\n",
    "        filtered_instances = sum(filtered_word_frequencies.values())\n",
    "        excluded_words = len(word_frequencies) - len(unique_words)\n",
    "        excluded_instances = total_instances - filtered_instances\n",
    "\n",
    "        print(f\"Total word instances in text: {total_instances:,}\")\n",
    "        print(f\"Words shorter than {min_word_length} characters excluded: {excluded_words:,} unique words ({excluded_instances:,} instances)\")\n",
    "        print(f\"Unique words to analyze: {len(unique_words):,} (after filtering)\")\n",
    "        print(f\"Instances to analyze: {filtered_instances:,} (after filtering)\")\n",
    "\n",
    "        # Store results\n",
    "        word_results = []\n",
    "\n",
    "        # Percentiles for frequency ranking\n",
    "        frequencies = list(filtered_word_frequencies.values())\n",
    "\n",
    "        # Validate each unique word\n",
    "        for word in unique_words:\n",
    "            is_valid, errors, analysis_data = self.validate_word(word)\n",
    "            freq = filtered_word_frequencies[word]\n",
    "\n",
    "            # Compute metrics\n",
    "            frequency_percentile = (sum(1 for f in frequencies if f <= freq) / len(frequencies))\n",
    "            grammar_score = 1.0 if is_valid else max(0.0, 1.0 - (len(errors) * 0.2))\n",
    "            structural_complexity = analysis_data.get('max_density', 0) / 3.0\n",
    "\n",
    "            density_profile = analysis_data.get('density_profile', [])\n",
    "            density_variance = np.var(density_profile) if density_profile else 0.0\n",
    "            density_mean = np.mean(density_profile) if density_profile else 0.0\n",
    "\n",
    "            # Build DataFrame row\n",
    "            row = {\n",
    "                'word': word,\n",
    "                'frequency': freq,\n",
    "                'frequency_percentile': round(frequency_percentile, 2),\n",
    "                'word_length': len(word),\n",
    "                'token_count': analysis_data.get('token_count', 0),\n",
    "                'is_valid': is_valid,\n",
    "                'grammar_score': round(grammar_score, 4),\n",
    "                'error_count': len(errors),\n",
    "                'structural_complexity': round(structural_complexity, 4),\n",
    "                'density_variance': round(density_variance, 4),\n",
    "                'density_mean': round(density_mean, 4),\n",
    "                'max_density': analysis_data.get('max_density', 0),\n",
    "                'core_index': analysis_data.get('core_index', -1),\n",
    "                'gallows_count': analysis_data.get('gallows_count', 0),\n",
    "                'benches_count': analysis_data.get('benches_count', 0),\n",
    "                'loops_count': analysis_data.get('loops_count', 0),\n",
    "                'classification': analysis_data.get('classification', 'non_compliant'),\n",
    "                'errors': ','.join(errors) if errors else '',\n",
    "                'density_profile': str(analysis_data.get('density_profile', [])),\n",
    "                'tokens': ','.join(analysis_data.get('tokens', []))\n",
    "            }\n",
    "\n",
    "            word_results.append(row)\n",
    "\n",
    "        # Create DataFrame from results\n",
    "        df = pd.DataFrame(word_results)\n",
    "\n",
    "        # Generate stats\n",
    "        consolidated_stats = self.generate_consolidated_stats(df, total_instances, filtered_instances)\n",
    "\n",
    "        return df, consolidated_stats\n",
    "\n",
    "    def generate_consolidated_stats(self, df, total_instances, filtered_instances):\n",
    "        \"\"\"\n",
    "        Compute summary statistics from the analysis DataFrame.\n",
    "        - Includes compliance rates, averages, distributions, etc.\n",
    "        - Focuses on unique words, with instance-weighted metrics for context.\n",
    "        \"\"\"\n",
    "        unique_words = len(df)\n",
    "        valid_unique_words = df['is_valid'].sum()\n",
    "        invalid_unique_words = unique_words - valid_unique_words\n",
    "\n",
    "        # Instance-based (frequency-weighted)\n",
    "        valid_instances = df[df['is_valid'] == True]['frequency'].sum()\n",
    "        invalid_instances = df[df['is_valid'] == False]['frequency'].sum()\n",
    "\n",
    "        # Collect all errors\n",
    "        all_errors = []\n",
    "        for errors_str in df['errors']:\n",
    "            if errors_str:\n",
    "                all_errors.extend(errors_str.split(','))\n",
    "\n",
    "        error_counts = Counter(all_errors)\n",
    "\n",
    "        # Compile stats dict\n",
    "        stats = {\n",
    "            'total_instances_original': total_instances,\n",
    "            'filtered_instances_analyzed': filtered_instances,\n",
    "            'excluded_instances': total_instances - filtered_instances,\n",
    "            'filtering_retention_rate': (filtered_instances / total_instances * 100) if total_instances > 0 else 0,\n",
    "\n",
    "            'unique_words': unique_words,\n",
    "            'valid_unique_words': valid_unique_words,\n",
    "            'invalid_unique_words': invalid_unique_words,\n",
    "            'unique_word_compliance_rate': (valid_unique_words / unique_words * 100) if unique_words > 0 else 0,\n",
    "\n",
    "            'total_instances': filtered_instances,\n",
    "            'valid_instances': valid_instances,\n",
    "            'invalid_instances': invalid_instances,\n",
    "            'instance_compliance_rate': (valid_instances / filtered_instances * 100) if filtered_instances > 0 else 0,\n",
    "\n",
    "            'avg_word_length': df['word_length'].mean(),\n",
    "            'min_word_length': df['word_length'].min(),\n",
    "            'max_word_length': df['word_length'].max(),\n",
    "            'median_word_length': df['word_length'].median(),\n",
    "\n",
    "            'avg_token_count': df['token_count'].mean(),\n",
    "            'min_token_count': df['token_count'].min(),\n",
    "            'max_token_count': df['token_count'].max(),\n",
    "\n",
    "            'avg_max_density': df['max_density'].mean(),\n",
    "            'core_density_distribution': df['max_density'].value_counts().to_dict(),\n",
    "\n",
    "            'total_gallows_in_vocab': df['gallows_count'].sum(),\n",
    "            'total_benches_in_vocab': df['benches_count'].sum(),\n",
    "            'total_loops_in_vocab': df['loops_count'].sum(),\n",
    "            'avg_gallows_per_word': df['gallows_count'].mean(),\n",
    "            'avg_benches_per_word': df['benches_count'].mean(),\n",
    "            'avg_loops_per_word': df['loops_count'].mean(),\n",
    "\n",
    "            'error_types': dict(error_counts),\n",
    "            'most_common_error': error_counts.most_common(1)[0] if error_counts else None,\n",
    "            'avg_errors_per_word': df['error_count'].mean(),\n",
    "\n",
    "            'validity_by_length': df.groupby('word_length')['is_valid'].agg(['count', 'sum', 'mean']).to_dict(),\n",
    "\n",
    "            'most_frequent_words': df.nlargest(10, 'frequency')[['word', 'frequency', 'is_valid']].to_dict('records'),\n",
    "            'avg_frequency': df['frequency'].mean(),\n",
    "            'median_frequency': df['frequency'].median(),\n",
    "\n",
    "            'sample_valid_words': df[df['is_valid'] == True]['word'].head(10).tolist(),\n",
    "            'sample_invalid_words': df[df['is_valid'] == False][['word', 'errors', 'frequency']].head(10).to_dict('records')\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "\n",
    "def display_grammar_results(df, sort_by='frequency', ascending=False, max_rows=50):\n",
    "    \"\"\"\n",
    "    Display analysis results in a formatted table.\n",
    "    - Configures pandas for display.\n",
    "    - Selects key columns.\n",
    "    - Sorts and prints top rows.\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 15)\n",
    "    pd.set_option('display.precision', 4)\n",
    "\n",
    "    display_columns = [\n",
    "        'word',\n",
    "        'frequency',\n",
    "        'frequency_percentile',\n",
    "        'grammar_score',\n",
    "        'structural_complexity',\n",
    "        'density_variance',\n",
    "        'density_mean',\n",
    "        'word_length',\n",
    "        'is_valid',\n",
    "        'classification',\n",
    "        'error_count'\n",
    "    ]\n",
    "\n",
    "    df_sorted = df.sort_values(sort_by, ascending=ascending)\n",
    "    result_df = df_sorted[display_columns].head(max_rows)\n",
    "\n",
    "    print(f\"VOYNICH GRAMMAR ANALYSIS (sorted by {sort_by})\")\n",
    "    print(\"=\" * 120)\n",
    "    print(result_df.to_string(index=True, max_rows=max_rows))\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# functions for displaying subsets\n",
    "def get_top_frequent_words(df, n=20):\n",
    "    \"\"\"Display top N frequent words.\"\"\"\n",
    "    return display_grammar_results(df, sort_by='frequency', ascending=False, max_rows=n)\n",
    "\n",
    "def get_most_grammatical_words(df, n=20):\n",
    "    \"\"\"Display words with highest grammar scores.\"\"\"\n",
    "    return display_grammar_results(df, sort_by='grammar_score', ascending=False, max_rows=n)\n",
    "\n",
    "def get_invalid_words(df, n=20):\n",
    "    \"\"\"Display invalid words by frequency.\"\"\"\n",
    "    invalid_df = df[df['is_valid'] == False]\n",
    "    return display_grammar_results(invalid_df, sort_by='frequency', ascending=False, max_rows=n)\n",
    "\n",
    "\n",
    "def analyze_voynich_grammar(filepath=\"transliteration_zl.txt\", treat_commas_as_spaces=True, min_word_length=2, display_results=True):\n",
    "\n",
    "    validator = VoynichGrammarValidator()\n",
    "    df, stats = validator.analyze_voynich_text(filepath, treat_commas_as_spaces, min_word_length)\n",
    "\n",
    "    if display_results and df is not None:\n",
    "        print(\"=\" * 100)\n",
    "        print(\"VOYNICH GRAMMAR ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        print(f\"Original total instances: {stats['total_instances_original']:,}\")\n",
    "        print(f\"Instances after filtering (≥{min_word_length} chars): {stats['filtered_instances_analyzed']:,}\")\n",
    "        print(f\"Filtering retention rate: {stats['filtering_retention_rate']:.1f}%\")\n",
    "        print(f\"Unique words analyzed: {stats['unique_words']:,}\")\n",
    "        print(f\"Grammar compliance rate: {stats['unique_word_compliance_rate']:.1f}%\")\n",
    "        print(f\"Most common error: {stats['most_common_error']}\")\n",
    "        print()\n",
    "\n",
    "        get_top_frequent_words(df, 20)\n",
    "\n",
    "    return df, stats\n",
    "\n",
    "df, stats = analyze_voynich_grammar(\"transliteration_zl.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
